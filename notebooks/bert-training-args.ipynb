{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-03T22:48:15.469611Z","iopub.status.busy":"2024-05-03T22:48:15.468757Z","iopub.status.idle":"2024-05-03T22:48:35.679913Z","shell.execute_reply":"2024-05-03T22:48:35.679120Z","shell.execute_reply.started":"2024-05-03T22:48:15.469553Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-05-03 22:48:25.769668: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-05-03 22:48:25.769765: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-05-03 22:48:25.928215: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import os\n","import numpy as np\n","import torch \n","\n","from transformers import AutoTokenizer, AutoModelForTokenClassification, Trainer, TrainingArguments\n","from seqeval.metrics import recall_score, precision_score, accuracy_score\n","from functools import partial"]},{"cell_type":"markdown","metadata":{},"source":["Dataloader"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T22:48:35.682790Z","iopub.status.busy":"2024-05-03T22:48:35.681952Z","iopub.status.idle":"2024-05-03T22:48:35.705432Z","shell.execute_reply":"2024-05-03T22:48:35.704466Z","shell.execute_reply.started":"2024-05-03T22:48:35.682750Z"},"trusted":true},"outputs":[],"source":["from datasets import Dataset\n","from functools import partial, reduce\n","from transformers import AutoTokenizer\n","from pandas import read_json, read_csv\n","import os\n","from tqdm import tqdm\n","\n","\n","def encode_labels(example, label2id):\n","    \"\"\"\n","    Encodes the labels into integers\n","    to be used with datasets.map() with batched=False\n","    \n","    Encodes the labels into integers.\n","    \n","    \"\"\"\n","    labels = example['labels']\n","    encoded = [label2id[label] for label in labels]\n","    return {'labels': encoded}\n","\n","\n","def tokenize_and_align(example, tokenizer, with_labels=True, overlap_size=0):\n","    \"\"\"\n","    Tokenizes the input and aligns the labels with the tokens\n","    To be used with datasets.map() with batched=False\n","\n","    Takes in \n","        - example : an example from the datasets class\n","        - overlap_size: the number of tokens that overlap between two consecutive chunks\n","        \n","    outputs:\n","        - a Dict[]->List with columns:\n","            - of the bert tokenizer output\n","            - encoded labels\n","    \"\"\"\n","\n","    if with_labels:\n","        org_labels = example['labels']\n","\n","    tokenized_inputs = tokenizer(example['tokens'], is_split_into_words=True, return_offsets_mapping=True, truncation=True, padding='max_length', max_length=512, return_overflowing_tokens=True, stride=overlap_size, return_tensors='pt')\n","    tokenized_inputs.pop('overflow_to_sample_mapping')\n","    tokenized_inputs.pop('offset_mapping')\n","    \n","    new_labels = []\n","    org_word_ids_list = []\n","    document_id = []\n","    \n","    # Iterating over chunks\n","    for i, chunk in enumerate(tokenized_inputs['input_ids']):\n","        ids_of_tokens = tokenized_inputs.word_ids(i)\n","        \n","        org_word_ids_list.append(ids_of_tokens)\n","        document_id.append(example['document'])\n","\n","        if with_labels:\n","            # Iterating over ids of tokens\n","            chunk_labels = []\n","            for id in ids_of_tokens:\n","                # if id=None, then it means it's some BERT token (CLS, SEP or PAD)\n","                if id is None:\n","                    chunk_labels.append(-100)\n","                else:\n","                    chunk_labels.append(org_labels[id])\n","            new_labels.append(chunk_labels)\n","\n","    if with_labels:\n","        tokenized_inputs['labels'] = new_labels\n","    \n","    tokenized_inputs['org_word_ids'] = org_word_ids_list\n","    tokenized_inputs['document'] = document_id\n","\n","    return tokenized_inputs\n","\n","\n","def flatten_data(data, keys_to_flatten):\n","    \"\"\"\n","    Flattens the rows of the datasets object for the keys_to_flatten columns\n","\n","    Takes in:\n","        - data: a dataset object\n","        - keys_to_flatten: a list with the keys to flatten\n","    Outputs:\n","        - a dataset object with the keys_to_flatten columns\n","    \"\"\"\n","\n","    data_flat = {}\n","\n","    for key in tqdm(keys_to_flatten):\n","        data_flat[key] = reduce(lambda x, y: x + y, data[key])\n","\n","    return Dataset.from_dict(data_flat)\n","\n","\n","def preprocess_data(data, tokenizer, label2id={}, with_labels=True, overlap_size=0, keys_to_flatten=['input_ids', 'token_type_ids', 'attention_mask', 'org_word_ids', 'document']):\n","    \"\"\"\n","    Preprocesses the data\n","    \n","    Takes in \n","        - data: a dataset object with columns 'document', 'tokens' (if with_labels=True, also has to have 'labels')\n","        - tokenizer: a tokenizer object\n","        - label2id: a dictionary with the labels and their corresponding ids. If with_labels=True, this has to be provided. By default, it's an empty dictionary.\n","        - with_labels: a boolean indicating if the data has labels. By default, it's True.\n","        - overlap_size: the number of tokens that overlap between two consecutive chunks. By default, it's 0.\n","        - keys_to_flatten : a list of columns to keep in the output dataset. By default, it's ['input_ids', 'token_type_ids', 'attention_mask', 'org_word_ids', 'document']\n","        \n","    outputs:\n","        - a dataset object with keys_to_flatten columns\n","    \"\"\"\n","\n","    assert 'document' in data.column_names, \"data has to have a 'document' column\"\n","    assert 'tokens' in data.column_names, \"data has to have a 'tokens' column\"\n","    if with_labels:\n","        assert 'labels' in data.column_names, \"data has to have a 'labels' column\"\n","        assert label2id, \"label2id has to be provided if with_labels=True\"\n","\n","    if with_labels:\n","        keys_to_flatten.append('labels')\n","\n","        print(\"encoding the labels...\")\n","        data = data.map(partial(encode_labels, label2id=label2id), batched=False)\n","\n","    print(\"tokenizing and aligning...\")\n","    data = data.map(partial(tokenize_and_align, tokenizer=tokenizer, overlap_size=overlap_size, with_labels=with_labels), batched=False)\n","\n","    print(\"flattening the data...\")\n","    data = flatten_data(data, keys_to_flatten)\n","    \n","    return data\n","\n","\n","def get_dataset_from_path(data_path):\n","    \"\"\"\n","    Loads a dataset from a path and returns it as a datasets object\n","\n","    Takes in \n","        - data: a string with the path to the data (has to be a json or csv file)\n","    \n","    outputs:\n","        - a datasets object\n","    \"\"\"\n","\n","    filetype = data_path.split('.')[-1]\n","    data = None\n","    if filetype == 'json':\n","        data = read_json(data_path)\n","    elif filetype == 'csv':\n","        data = read_csv(data_path)\n","    else:\n","        raise ValueError('Filetype not supported. Suuported filetypes are: json, csv')\n","    \n","    data = Dataset.from_pandas(data)\n","\n","    return data\n","\n","\n","def get_train_val_test_split(data, seed, val_size=0.1, test_size=0.1):\n","    \"\"\"\n","    Takes in:\n","        - data: a dataset object\n","        - seed: the seed for the random split\n","        - val_size: the size of the validation set\n","        - test_size: the size of the test set\n","    Outputs:\n","        - a tuple with data_train, data_val, data_test\n","    \"\"\"\n","\n","    data = data.train_test_split(test_size=test_size, seed=seed)\n","    data_train_val = data['train'].train_test_split(test_size=val_size, seed=seed)\n","\n","    return data_train_val['train'], data_train_val['test'], data['test']\n"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T22:48:51.647174Z","iopub.status.busy":"2024-05-03T22:48:51.646785Z","iopub.status.idle":"2024-05-03T22:48:51.656347Z","shell.execute_reply":"2024-05-03T22:48:51.655260Z","shell.execute_reply.started":"2024-05-03T22:48:51.647143Z"},"trusted":true},"outputs":[],"source":["def get_fbeta_score(precision, recall, beta=5.0):\n","    b2 = beta ** 2\n","    return (1 + b2) * ((precision * recall) / (b2 * precision + recall))\n","\n","\n","def compute_metrics(p, labels_list):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    # Remove ignored index (special tokens)\n","    true_predictions = [\n","        [labels_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [labels_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    recall = recall_score(true_labels, true_predictions)\n","    precision = precision_score(true_labels, true_predictions)\n","    fbeta_score = get_fbeta_score(precision, recall)\n","\n","    results = {\n","        'recall': recall,\n","        'precision': precision,\n","        'fbeta_score': fbeta_score\n","    }\n","        \n","    return results\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T22:48:53.506366Z","iopub.status.busy":"2024-05-03T22:48:53.505658Z","iopub.status.idle":"2024-05-03T22:48:53.629110Z","shell.execute_reply":"2024-05-03T22:48:53.628336Z","shell.execute_reply.started":"2024-05-03T22:48:53.506326Z"},"trusted":true},"outputs":[],"source":["# Model configuration\n","\n","class CFG:\n","    LABELS_LIST = ['B-NAME_STUDENT', 'B-EMAIL', 'B-USERNAME', 'B-ID_NUM', 'B-PHONE_NUM', 'B-URL_PERSONAL', 'B-STREET_ADDRESS', 'I-NAME_STUDENT', 'I-EMAIL', 'I-USERNAME', 'I-ID_NUM', 'I-PHONE_NUM','I-URL_PERSONAL','I-STREET_ADDRESS', 'O']\n","    label2id = {label: i for i, label in enumerate(LABELS_LIST)}\n","    label2id['[PAD]'] = -100\n","    id2label = {i: label for label, i in label2id.items()}\n","    seed = 42\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    torch.backends.cudnn.benchmark = True\n","\n","    # model checkpoint\n","    model_name = 'bert-base-uncased'\n","    train_head_only = False\n","\n","    # path to the directory where the model will be saved\n","    local_path = \"/kaggle/working/\"\n","    target_dir = os.path.join(local_path,'..','models', 'bert')\n","    model_save_path = os.path.join(target_dir, 'lr'), \n","\n","    #training arguments\n","    training_args = TrainingArguments(\n","        output_dir=os.path.join(target_dir, 'trainer_args'), \n","        evaluation_strategy=\"epoch\", \n","        learning_rate=1e-4, \n","        weight_decay=0.1, \n","        num_train_epochs=10,\n","        )"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T22:48:55.112580Z","iopub.status.busy":"2024-05-03T22:48:55.111961Z","iopub.status.idle":"2024-05-03T22:48:58.969501Z","shell.execute_reply":"2024-05-03T22:48:58.968541Z","shell.execute_reply.started":"2024-05-03T22:48:55.112549Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6ff27f580fc046979f42211e826f23cf","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b49a8c46a844aa99f15af917dd9a114","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b45b89062c7645669e9980f54fe3b801","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"abd3821cd1814d1ab2d59d67dbacaac9","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d8e4857fea72439c96c788e553c37d07","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n","model = AutoModelForTokenClassification.from_pretrained(CFG.model_name, num_labels=len(CFG.id2label), id2label=CFG.id2label, label2id=CFG.label2id)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T22:49:00.555885Z","iopub.status.busy":"2024-05-03T22:49:00.555139Z","iopub.status.idle":"2024-05-03T22:50:46.002999Z","shell.execute_reply":"2024-05-03T22:50:46.001940Z","shell.execute_reply.started":"2024-05-03T22:49:00.555852Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["encoding the labels...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c877880ba964050b332ed2a5bf56f87","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["tokenizing and aligning...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5f7c3d4000884635a63c0baff4f8c00d","version_major":2,"version_minor":0},"text/plain":["Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["flattening the data...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 6/6 [00:23<00:00,  3.99s/it]\n"]}],"source":["data_path = \"/kaggle/input/pii-detection-removal-from-educational-data/train.json\"\n","data = get_dataset_from_path(data_path)\n","data = preprocess_data(data, tokenizer, label2id = CFG.label2id)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T22:51:42.098711Z","iopub.status.busy":"2024-05-03T22:51:42.097995Z","iopub.status.idle":"2024-05-03T22:51:42.128602Z","shell.execute_reply":"2024-05-03T22:51:42.127717Z","shell.execute_reply.started":"2024-05-03T22:51:42.098676Z"},"trusted":true},"outputs":[],"source":["data_train, data_eval, data_test = get_train_val_test_split(data, seed=CFG.seed)"]},{"cell_type":"markdown","metadata":{},"source":["Training the BERT model with lr=0.0001"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T22:51:44.331347Z","iopub.status.busy":"2024-05-03T22:51:44.330959Z","iopub.status.idle":"2024-05-03T22:51:44.662357Z","shell.execute_reply":"2024-05-03T22:51:44.661568Z","shell.execute_reply.started":"2024-05-03T22:51:44.331317Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n","dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n","  warnings.warn(\n"]}],"source":["trainer = Trainer(\n","    model=model,\n","    args=CFG.training_args,\n","    train_dataset=data_train,\n","    eval_dataset=data_eval,\n","    tokenizer=tokenizer,\n","    compute_metrics=partial(compute_metrics, labels_list=CFG.LABELS_LIST),\n",")"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-03T22:51:46.582838Z","iopub.status.busy":"2024-05-03T22:51:46.582478Z","iopub.status.idle":"2024-05-04T00:34:31.314585Z","shell.execute_reply":"2024-05-04T00:34:31.313616Z","shell.execute_reply.started":"2024-05-03T22:51:46.582808Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='6490' max='6490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [6490/6490 1:42:40, Epoch 10/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Recall</th>\n","      <th>Precision</th>\n","      <th>Fbeta Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.018900</td>\n","      <td>0.003188</td>\n","      <td>0.059946</td>\n","      <td>0.349206</td>\n","      <td>0.061918</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.003700</td>\n","      <td>0.001435</td>\n","      <td>0.749319</td>\n","      <td>0.766017</td>\n","      <td>0.749948</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.002500</td>\n","      <td>0.002142</td>\n","      <td>0.534060</td>\n","      <td>0.771654</td>\n","      <td>0.540460</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.000600</td>\n","      <td>0.001756</td>\n","      <td>0.814714</td>\n","      <td>0.753149</td>\n","      <td>0.812160</td>\n","    </tr>\n","    <tr>\n","      <td>5</td>\n","      <td>0.000500</td>\n","      <td>0.002034</td>\n","      <td>0.839237</td>\n","      <td>0.627291</td>\n","      <td>0.828471</td>\n","    </tr>\n","    <tr>\n","      <td>6</td>\n","      <td>0.000700</td>\n","      <td>0.002254</td>\n","      <td>0.809264</td>\n","      <td>0.671946</td>\n","      <td>0.802953</td>\n","    </tr>\n","    <tr>\n","      <td>7</td>\n","      <td>0.000200</td>\n","      <td>0.002013</td>\n","      <td>0.798365</td>\n","      <td>0.827684</td>\n","      <td>0.799454</td>\n","    </tr>\n","    <tr>\n","      <td>8</td>\n","      <td>0.000200</td>\n","      <td>0.002019</td>\n","      <td>0.803815</td>\n","      <td>0.833333</td>\n","      <td>0.804911</td>\n","    </tr>\n","    <tr>\n","      <td>9</td>\n","      <td>0.000100</td>\n","      <td>0.002191</td>\n","      <td>0.798365</td>\n","      <td>0.834758</td>\n","      <td>0.799706</td>\n","    </tr>\n","    <tr>\n","      <td>10</td>\n","      <td>0.000100</td>\n","      <td>0.002257</td>\n","      <td>0.811989</td>\n","      <td>0.818681</td>\n","      <td>0.812244</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n","/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/plain":["TrainOutput(global_step=6490, training_loss=0.0022503415045137583, metrics={'train_runtime': 6164.399, 'train_samples_per_second': 16.834, 'train_steps_per_second': 1.053, 'total_flos': 2.711819644747776e+16, 'train_loss': 0.0022503415045137583, 'epoch': 10.0})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T00:34:35.749094Z","iopub.status.busy":"2024-05-04T00:34:35.748681Z","iopub.status.idle":"2024-05-04T00:35:07.318034Z","shell.execute_reply":"2024-05-04T00:35:07.317052Z","shell.execute_reply.started":"2024-05-04T00:34:35.749063Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n","  warnings.warn('Was asked to gather along dimension 0, but all '\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='81' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [81/81 00:26]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["{'eval_loss': 0.00291440193541348,\n"," 'eval_recall': 0.778705636743215,\n"," 'eval_precision': 0.7936170212765957,\n"," 'eval_fbeta_score': 0.7792687826436321,\n"," 'eval_runtime': 31.5543,\n"," 'eval_samples_per_second': 40.628,\n"," 'eval_steps_per_second': 2.567,\n"," 'epoch': 10.0}"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["trainer.evaluate(data_test)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T00:45:04.915405Z","iopub.status.busy":"2024-05-04T00:45:04.914440Z","iopub.status.idle":"2024-05-04T00:45:04.919906Z","shell.execute_reply":"2024-05-04T00:45:04.918910Z","shell.execute_reply.started":"2024-05-04T00:45:04.915363Z"},"trusted":true},"outputs":[],"source":["lr_res = 'lr' + str(CFG.training_args.learning_rate)\n","model_save_path = os.path.join(CFG.target_dir, lr_res) "]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T00:47:43.151235Z","iopub.status.busy":"2024-05-04T00:47:43.150372Z","iopub.status.idle":"2024-05-04T00:47:44.676188Z","shell.execute_reply":"2024-05-04T00:47:44.675343Z","shell.execute_reply.started":"2024-05-04T00:47:43.151200Z"},"trusted":true},"outputs":[],"source":["trainer.save_model(model_save_path)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T00:51:14.342870Z","iopub.status.busy":"2024-05-04T00:51:14.342446Z","iopub.status.idle":"2024-05-04T00:51:14.376821Z","shell.execute_reply":"2024-05-04T00:51:14.375918Z","shell.execute_reply.started":"2024-05-04T00:51:14.342840Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f57e3f2953b44cf39e3e1bd1cef9d3f6","version_major":2,"version_minor":0},"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"]},"metadata":{},"output_type":"display_data"}],"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T00:51:43.501865Z","iopub.status.busy":"2024-05-04T00:51:43.501461Z","iopub.status.idle":"2024-05-04T00:51:56.653265Z","shell.execute_reply":"2024-05-04T00:51:56.652333Z","shell.execute_reply.started":"2024-05-04T00:51:43.501834Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"19863ac037f644318d21f8ba163d2d66","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/63.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3c15daf9a7eb475ca102a216411c0bad","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/436M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["CommitInfo(commit_url='https://huggingface.co/zeinab-sheikhi/bert-pii-detection-baseline/commit/f06fb1b2c1da6f63c061cd3b3ee3ada6d4b7acff', commit_message='Upload BertForTokenClassification', commit_description='', oid='f06fb1b2c1da6f63c061cd3b3ee3ada6d4b7acff', pr_url=None, pr_revision=None, pr_num=None)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["token = \"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\"\n","model.push_to_hub(\"zeinab-sheikhi/bert-pii-detection-baseline\", token=token)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":7500999,"sourceId":66653,"sourceType":"competition"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":4}
