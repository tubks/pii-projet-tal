{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import Dataset\n",
    "from functools import reduce\n",
    "from seqeval.metrics import recall_score, precision_score\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from functools import partial\n",
    "\n",
    "# add the parent directory to the path so we can import the dataloader module\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src.data.dataloader import preprocess_data, get_dataset_from_path, get_train_val_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roberta\n",
    " The inputs of the model take pieces of 512 contiguous tokens that may span over documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    LABELS_LIST = ['B-NAME_STUDENT', 'B-EMAIL', 'B-USERNAME', 'B-ID_NUM', 'B-PHONE_NUM', 'B-URL_PERSONAL', 'B-STREET_ADDRESS', 'I-NAME_STUDENT', 'I-EMAIL', 'I-USERNAME', 'I-ID_NUM', 'I-PHONE_NUM','I-URL_PERSONAL','I-STREET_ADDRESS', 'O']\n",
    "    label2id = {label: i for i, label in enumerate(LABELS_LIST)}\n",
    "    label2id['[PAD]'] = -100\n",
    "    id2label = {i: label for label, i in label2id.items()}\n",
    "    seed = 42\n",
    "\n",
    "    # model checkpoint\n",
    "    model_name = 'roberta-base'\n",
    "    train_head_only = False\n",
    "\n",
    "    # path to the directory where the model will be saved\n",
    "    local_path = os.path.abspath(os.path.abspath(''))\n",
    "    target_dir = os.path.join(local_path,'..','models', 'roberta-base')\n",
    "\n",
    "    #training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=os.path.join(target_dir, 'trainer'), \n",
    "        evaluation_strategy=\"epoch\"\n",
    "        )\n",
    "    model_save_path = os.path.join(target_dir, 'model')\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name, add_prefix_space=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding the labels...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b70477f56a4cc2a3382909d6f8caa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing and aligning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fa0a852f2a0404ca06fae885d1e627d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flattening the data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:06<00:00,  1.12s/it]\n"
     ]
    }
   ],
   "source": [
    "keys_to_flatten = ['labels', 'input_ids', 'attention_mask', 'org_word_ids','document']\n",
    "data_path = os.path.join('..','data', 'raw', 'train.json')\n",
    "data = get_dataset_from_path(data_path)\n",
    "data = preprocess_data(data, tokenizer, label2id = CFG.label2id, keys_to_flatten=keys_to_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fbeta_score(precision, recall, beta=5.0):\n",
    "        b2 = beta ** 2\n",
    "        return (1 + b2) * ((precision * recall) / (b2 * precision + recall))\n",
    "\n",
    "def compute_metrics(p, labels_list):\n",
    "        predictions, labels = p\n",
    "        predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "        # Remove ignored index (special tokens)\n",
    "        \n",
    "        true_predictions = [\n",
    "            [labels_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "        true_labels = [\n",
    "            [labels_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "            for prediction, label in zip(predictions, labels)\n",
    "        ]\n",
    "\n",
    "        recall = recall_score(true_labels, true_predictions)\n",
    "        precision = precision_score(true_labels, true_predictions)\n",
    "        fbeta_score = get_fbeta_score(precision, recall)\n",
    "\n",
    "        results = {\n",
    "            'recall': recall,\n",
    "            'precision': precision,\n",
    "            'fbeta_score': fbeta_score\n",
    "            }\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    CFG.model_name, num_labels=len(CFG.id2label), id2label=CFG.id2label, label2id=CFG.label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training all layers\n"
     ]
    }
   ],
   "source": [
    "# Freezing the Roberta layers\n",
    "if CFG.train_head_only:\n",
    "    print('Training head only')\n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad = False\n",
    "else:\n",
    "    print('Training all layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_eval, data_test = get_train_val_test_split(data, seed=CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e2d1a6ee0747c081d884124ae7854b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4290 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory /Users/zofia/Documents/UNI/UPARIS/sem2/pii-projet-tal/notebooks/../models/roberta-base/trainer/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0195, 'learning_rate': 4.4172494172494175e-05, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory /Users/zofia/Documents/UNI/UPARIS/sem2/pii-projet-tal/notebooks/../models/roberta-base/trainer/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0036, 'learning_rate': 3.834498834498835e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad841f68ec194bc897756a55eafd1f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.002211528830230236, 'eval_recall': 0.6772151898734177, 'eval_precision': 0.4543524416135881, 'eval_fbeta_score': 0.6646756659897264, 'eval_runtime': 52.2848, 'eval_samples_per_second': 24.309, 'eval_steps_per_second': 3.041, 'epoch': 1.0}\n",
      "{'loss': 0.0022, 'learning_rate': 3.251748251748252e-05, 'epoch': 1.05}\n",
      "{'loss': 0.0016, 'learning_rate': 2.6689976689976692e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0013, 'learning_rate': 2.0862470862470865e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3933a32142fb476ba8ea5d705bc1b4cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0008185721817426383, 'eval_recall': 0.9113924050632911, 'eval_precision': 0.7146401985111662, 'eval_fbeta_score': 0.9018427074551367, 'eval_runtime': 51.2759, 'eval_samples_per_second': 24.787, 'eval_steps_per_second': 3.101, 'epoch': 2.0}\n",
      "{'loss': 0.0009, 'learning_rate': 1.5034965034965034e-05, 'epoch': 2.1}\n",
      "{'loss': 0.0007, 'learning_rate': 9.207459207459208e-06, 'epoch': 2.45}\n",
      "{'loss': 0.0005, 'learning_rate': 3.3799533799533803e-06, 'epoch': 2.8}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463239333f534cf8bd37cfde7f4bbb75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0006997723248787224, 'eval_recall': 0.9208860759493671, 'eval_precision': 0.7718832891246684, 'eval_fbeta_score': 0.9140993113446902, 'eval_runtime': 49.5996, 'eval_samples_per_second': 25.625, 'eval_steps_per_second': 3.206, 'epoch': 3.0}\n",
      "{'train_runtime': 4926.483, 'train_samples_per_second': 6.966, 'train_steps_per_second': 0.871, 'train_loss': 0.0035678880979547966, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4290, training_loss=0.0035678880979547966, metrics={'train_runtime': 4926.483, 'train_samples_per_second': 6.966, 'train_steps_per_second': 0.871, 'train_loss': 0.0035678880979547966, 'epoch': 3.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=CFG.training_args,\n",
    "    train_dataset=data_train,\n",
    "    eval_dataset=data_eval,\n",
    "    compute_metrics=partial(compute_metrics, labels_list=CFG.LABELS_LIST),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272b92f3259243d9913381085486a651",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.0008325826493091881,\n",
       " 'test_recall': 0.8086734693877551,\n",
       " 'test_precision': 0.8386243386243386,\n",
       " 'test_fbeta_score': 0.8097858125368441,\n",
       " 'test_runtime': 55.2545,\n",
       " 'test_samples_per_second': 25.573,\n",
       " 'test_steps_per_second': 3.203,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(data_test, metric_key_prefix='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(CFG.model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_from_huggingface = AutoModelForTokenClassification.from_pretrained('zeinab-sheikhi/Roberta-pii-detection-baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "model_from_huggingface.to('cpu')\n",
    "with torch.no_grad():\n",
    "    p = model_from_huggingface(torch.tensor(data['input_ids'][:10]), return_dict=True)\n",
    "    p = (p.logits, torch.tensor(data['labels'][:10]))\n",
    "    r = compute_metrics(p, CFG.LABELS_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p[0], axis=2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2407592cd57c4064a6f0aa2934160156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/177 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.0002600570733193308,\n",
       " 'eval_recall': 0.9948979591836735,\n",
       " 'eval_precision': 0.9048723897911833,\n",
       " 'eval_fbeta_score': 0.9911054637865312,\n",
       " 'eval_runtime': 55.14,\n",
       " 'eval_samples_per_second': 25.626,\n",
       " 'eval_steps_per_second': 3.21}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_from_huggingface = Trainer(\n",
    "    model=model_from_huggingface,\n",
    "    compute_metrics=partial(compute_metrics, labels_list=CFG.LABELS_LIST),\n",
    ")\n",
    "\n",
    "trainer_from_huggingface.evaluate(data_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
