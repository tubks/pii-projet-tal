{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "# from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roberta\n",
    " The inputs of the model take pieces of 512 contiguous tokens that may span over documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbd0e7ecb884a5e95ad8731d7d93703",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'roberta-base'\n",
    "\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = RobertaModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('..','data', 'raw', 'train.json')\n",
    "\n",
    "data = pd.read_json(data_path)\n",
    "\n",
    "data_dataset = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-NAME_STUDENT',\n",
       " 1: 'B-EMAIL',\n",
       " 2: 'B-USERNAME',\n",
       " 3: 'B-ID_NUM',\n",
       " 4: 'B-PHONE_NUM',\n",
       " 5: 'B-URL_PERSONAL',\n",
       " 6: 'B-STREET_ADDRESS',\n",
       " 7: 'I-NAME_STUDENT',\n",
       " 8: 'I-EMAIL',\n",
       " 9: 'I-USERNAME',\n",
       " 10: 'I-ID_NUM',\n",
       " 11: 'I-PHONE_NUM',\n",
       " 12: 'I-URL_PERSONAL',\n",
       " 13: 'I-STREET_ADDRESS',\n",
       " 14: 'O',\n",
       " -100: '[PAD]'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS_LIST = ['B-NAME_STUDENT', 'B-EMAIL', 'B-USERNAME', 'B-ID_NUM', 'B-PHONE_NUM', 'B-URL_PERSONAL', 'B-STREET_ADDRESS', 'I-NAME_STUDENT', 'I-EMAIL', 'I-USERNAME', 'I-ID_NUM', 'I-PHONE_NUM','I-URL_PERSONAL','I-STREET_ADDRESS', 'O']\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(LABELS_LIST)}\n",
    "label2id['[PAD]'] = -100\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(example):\n",
    "    \"\"\"\n",
    "    to be used with datasets.map() with batched=False\n",
    "    \n",
    "    Encodes the labels into integers.\n",
    "    \n",
    "    \"\"\"\n",
    "    labels = example['labels']\n",
    "    encoded = [label2id[label] for label in labels]\n",
    "    return {'labels': encoded}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c75af24f5f41c28773f5ba8e1bd6ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data_labels_encoded = data_dataset.map(encode_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align(example, overlap_size = 0):\n",
    "\n",
    "    org_labels = example['labels']\n",
    "    tokenized_inputs = tokenizer(example['full_text'][0], is_split_into_words=True, truncation=True, padding='max_length', return_overflowing_tokens=True, stride=overlap_size, return_tensors='pt')\n",
    "    \n",
    "    new_labels = []\n",
    "    org_word_ids_list = []\n",
    "    document_id = []\n",
    "    \n",
    "    #iterating over chunks\n",
    "    for i, chunk in enumerate(tokenized_inputs['input_ids']):\n",
    "        ids_of_tokens = tokenized_inputs.word_ids(i)\n",
    "        \n",
    "        org_word_ids_list.append(ids_of_tokens)\n",
    "        document_id.append(example['document'])\n",
    "        #iterating over ids of tokens\n",
    "        chunk_labels = []\n",
    "        for id in ids_of_tokens:\n",
    "            #if id=None, then it means it's some BERT token (CLS, SEP or PAD)\n",
    "            if id is None:\n",
    "                chunk_labels.append(-100)\n",
    "            else:\n",
    "                chunk_labels.append(org_labels[id])\n",
    "        new_labels.append(chunk_labels)\n",
    "\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    tokenized_inputs['org_word_ids'] = org_word_ids_list\n",
    "    tokenized_inputs['document'] = document_id\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = data_dataset['full_text'][0]\n",
    "tokenized_inputs = tokenizer(example, truncation=True, padding=True, return_overflowing_tokens=True, return_tensors='pt')\n",
    "# tokenized_input = tokenizer(example, is_split_into_words=True)\n",
    "# tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "# tokenizer.decode(tokenizer.convert_tokens_to_ids(tokens))\n",
    "len(tokenized_inputs['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_inputs.word_ids(0)\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# Assuming 'tokenizer' is your tokenizer object\n",
    "if isinstance(tokenizer, PreTrainedTokenizerFast):\n",
    "    print(\"This is a fast tokenizer.\")\n",
    "else:\n",
    "    print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
