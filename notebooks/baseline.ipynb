{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook contains the exploration and initial code for the baseline model. The corrected and ready-to-use version will be in src/baseline.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import BertTokenizer, BertForTokenClassification\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "data_path = os.path.join('..','data', 'raw', 'train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForTokenClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(text, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "    for word, label in zip(text, text_labels):\n",
    "        #tokenizes the word using BERT's subword tokenizer\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        #adds the same label to all the subwords of the word\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "11\n",
      "(['d', 'e', 's', 'i', 'g', 'n', '[MASK]', 't', 'h', 'i', 'n'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'I-NAME_STUDENT'])\n"
     ]
    }
   ],
   "source": [
    "sen = \"Design Thinking for innovation reflexion-Avril 2021-Nathalie Sylla\"\n",
    "labels = [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"O\", \"B-NAME_STUDENT\",\"I-NAME_STUDENT\"]\n",
    "print(len(sen.split()))\n",
    "print(len(labels))\n",
    "sen_tok= tokenize_and_preserve_labels(sen, labels)\n",
    "print(sen_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Design, Thinking, for, innovation, reflexion,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Diego, Estrada, \\n\\n, Design, Thinking, Assig...</td>\n",
       "      <td>[B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Reporting, process, \\n\\n, by, Gilberto, Gambo...</td>\n",
       "      <td>[O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Design, Thinking, for, Innovation, \\n\\n, Sind...</td>\n",
       "      <td>[O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Assignment, :,   , Visualization,  , Reflecti...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6802</th>\n",
       "      <td>[EXAMPLE, –, JOURNEY, MAP, \\n\\n, THE, CHALLENG...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6803</th>\n",
       "      <td>[Why, Mind, Mapping, ?, \\n\\n, Mind, maps, are,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6804</th>\n",
       "      <td>[Challenge, \\n\\n, So, ,, a, few, months, back,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6805</th>\n",
       "      <td>[Brainstorming, \\n\\n, Challenge, &amp;, Selection,...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6806</th>\n",
       "      <td>[Mind, Mapping, \\n\\n, Challenge, \\n\\n, My, con...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6807 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 tokens  \\\n",
       "0     [Design, Thinking, for, innovation, reflexion,...   \n",
       "1     [Diego, Estrada, \\n\\n, Design, Thinking, Assig...   \n",
       "2     [Reporting, process, \\n\\n, by, Gilberto, Gambo...   \n",
       "3     [Design, Thinking, for, Innovation, \\n\\n, Sind...   \n",
       "4     [Assignment, :,   , Visualization,  , Reflecti...   \n",
       "...                                                 ...   \n",
       "6802  [EXAMPLE, –, JOURNEY, MAP, \\n\\n, THE, CHALLENG...   \n",
       "6803  [Why, Mind, Mapping, ?, \\n\\n, Mind, maps, are,...   \n",
       "6804  [Challenge, \\n\\n, So, ,, a, few, months, back,...   \n",
       "6805  [Brainstorming, \\n\\n, Challenge, &, Selection,...   \n",
       "6806  [Mind, Mapping, \\n\\n, Challenge, \\n\\n, My, con...   \n",
       "\n",
       "                                                 labels  \n",
       "0     [O, O, O, O, O, O, O, O, O, B-NAME_STUDENT, I-...  \n",
       "1     [B-NAME_STUDENT, I-NAME_STUDENT, O, O, O, O, O...  \n",
       "2     [O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT, O...  \n",
       "3     [O, O, O, O, O, B-NAME_STUDENT, I-NAME_STUDENT...  \n",
       "4     [O, O, O, O, O, O, O, O, O, O, O, O, B-NAME_ST...  \n",
       "...                                                 ...  \n",
       "6802  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "6803  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "6804  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "6805  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "6806  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "\n",
       "[6807 rows x 2 columns]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[['tokens','labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_texts_and_labels = [tokenize_and_preserve_labels(sent, labs) for sent, labs in zip(data['tokens'].head(100), data['labels'])]\n",
    "tokenized_texts = [token_label_pair[0] for token_label_pair in tokenized_texts_and_labels]\n",
    "labels = [token_label_pair[1] for token_label_pair in tokenized_texts_and_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['design', 'thinking', 'for', 'innovation', 'reflex', '##ion', '-', 'av', '##ril', '2021', '-', 'nat', '##hal', '##ie', 'sy'] ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-NAME_STUDENT', 'B-NAME_STUDENT', 'B-NAME_STUDENT', 'I-NAME_STUDENT']\n",
      "809 809\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_texts_and_labels[0][0][:15],tokenized_texts_and_labels[0][1][:15])\n",
    "print(len(tokenized_texts_and_labels[0][0]),len(tokenized_texts_and_labels[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunker(tokens, labels, max_len=512, pad_text = 0.0,pad_labels=\"PAD\"):\n",
    "    \"\"\"\n",
    "    goal: chunk a text and encode it using a tokenizer\n",
    "    takens in:\n",
    "        tokens: list of tokens \n",
    "        labels: list of labels\n",
    "        max_len: int\n",
    "        pad_text: any type, by default: 0.0\n",
    "        pad_labels: any type, by default: \"PAD\" \n",
    "    outputs:\n",
    "        chunked_tokens: list of chunked tokens of the text\n",
    "        chunked_labels: list of chunked labels of the text\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(tokens)==len(labels)\n",
    "    chunked_tokens = []\n",
    "    chunked_labels = []\n",
    "    for pos in range(0,len(tokens),max_len):\n",
    "        pad_length = max_len - (len(tokens) % max_len)\n",
    "        tokens_chunk = tokens[pos:pos+max_len]\n",
    "        labels_chunk = labels[pos:pos+max_len]\n",
    "        if len(tokens_chunk) != 512:\n",
    "            tokens_chunk.extend(pad_length * [pad_text])\n",
    "            labels_chunk.extend(pad_length * [pad_labels])\n",
    "        chunked_tokens.append(tokens_chunk)\n",
    "        chunked_labels.append(labels_chunk)\n",
    "    return chunked_tokens,chunked_labels\n",
    "\n",
    "def chunk_text_and_labels(text_and_labels):\n",
    "    \"\"\"\n",
    "    goal: chunk a corpus of texts and encode it using a tokenizer\n",
    "    Takes in:\n",
    "        text_and_labels: list of tuples (tokens,labels)\n",
    "    outputs:\n",
    "        chunked_tokens: list of chunked tokens of all texts\n",
    "        chunked_labels: list of chunked labels of all texts\n",
    "    \"\"\"\n",
    "    all_chunked_tokens, all_chunked_labels = [],[]\n",
    "    for tokens, labels in text_and_labels:\n",
    "        chunked_tokens,chunked_labels = chunker(tokens,labels)\n",
    "        all_chunked_tokens.extend(chunked_tokens)\n",
    "        all_chunked_labels.extend(chunked_labels)\n",
    "    return all_chunked_tokens,all_chunked_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_tokens,chunked_labels=chunk_text_and_labels(tokenized_texts_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512, 512]\n"
     ]
    }
   ],
   "source": [
    "print([len(chunk) for chunk in chunked_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-ID_NUM', 'B-NAME_STUDENT', 'PAD', 'B-EMAIL', 'B-URL_PERSONAL', 'O', 'I-NAME_STUDENT'}\n",
      "{'B-ID_NUM': 0, 'B-NAME_STUDENT': 1, 'PAD': 2, 'B-EMAIL': 3, 'B-URL_PERSONAL': 4, 'O': 5, 'I-NAME_STUDENT': 6}\n",
      "2122\n"
     ]
    }
   ],
   "source": [
    "# getting the unique labels and adding PAD for padding\n",
    "label_names = set([label for row in labels for label in row])\n",
    "label_names.add('PAD')\n",
    "\n",
    "MAX_LEN = max(map(len, tokenized_texts))\n",
    "tag2idx = {t: i for i, t in enumerate(label_names)}\n",
    "print(label_names)\n",
    "print(tag2idx)\n",
    "print(MAX_LEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projet-en-tal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
