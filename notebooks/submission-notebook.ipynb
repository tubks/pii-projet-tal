{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66653,"databundleVersionId":7500999,"sourceType":"competition"},{"sourceId":8137423,"sourceType":"datasetVersion","datasetId":4810631},{"sourceId":36556,"sourceType":"modelInstanceVersion","modelInstanceId":30781}],"dockerImageVersionId":30684,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-22T19:06:11.910328Z","iopub.execute_input":"2024-04-22T19:06:11.910744Z","iopub.status.idle":"2024-04-22T19:06:13.330024Z","shell.execute_reply.started":"2024-04-22T19:06:11.910708Z","shell.execute_reply":"2024-04-22T19:06:13.328727Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/pii-fine-tuned/pytorch/v1/1/model.safetensors\n/kaggle/input/pii-test/test.json\n","output_type":"stream"}]},{"cell_type":"code","source":"from functools import reduce\nfrom datasets import Dataset\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:20:14.823261Z","iopub.execute_input":"2024-04-22T21:20:14.823966Z","iopub.status.idle":"2024-04-22T21:20:16.654332Z","shell.execute_reply.started":"2024-04-22T21:20:14.823917Z","shell.execute_reply":"2024-04-22T21:20:16.653502Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForTokenClassification\n\ntokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/pii-fine-tuned/transformers/v_with_config/1\")\nmodel = AutoModelForTokenClassification.from_pretrained(\"/kaggle/input/pii-fine-tuned/transformers/v_with_config/1\")","metadata":{"execution":{"iopub.status.busy":"2024-04-22T21:19:23.573675Z","iopub.execute_input":"2024-04-22T21:19:23.574772Z","iopub.status.idle":"2024-04-22T21:19:23.941058Z","shell.execute_reply.started":"2024-04-22T21:19:23.574734Z","shell.execute_reply":"2024-04-22T21:19:23.939998Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset('json', data_files='/kaggle/input/pii-test/test.json', split='train')\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T19:06:26.973315Z","iopub.execute_input":"2024-04-22T19:06:26.973987Z","iopub.status.idle":"2024-04-22T19:06:28.190745Z","shell.execute_reply.started":"2024-04-22T19:06:26.973952Z","shell.execute_reply":"2024-04-22T19:06:28.189865Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc6e3496827c4e0684fe9aa6db93d930"}},"metadata":{}}]},{"cell_type":"code","source":"def tokenize_and_align(example, overlap_size = 0):\n    \"\"\"\n    To be used with datasets.map() with batched=False\n\n    Takes in \n        - example : an example from the datasets class\n        - overlap_size: the number of tokens that overlap between two consecutive chunks\n        \n    outputs:\n        - a Dict[]->List with columns:\n            - of the bert tokenizer output\n            - encoded labels\n    \"\"\"\n\n    tokenized_inputs = tokenizer(example['tokens'], is_split_into_words=True, return_offsets_mapping=True, truncation=True, padding='max_length', max_length=512, return_overflowing_tokens=True, stride=overlap_size, return_tensors='pt')\n    tokenized_inputs.pop('overflow_to_sample_mapping')\n    tokenized_inputs.pop('offset_mapping')\n    \n    org_word_ids_list = []\n    document_id = []\n    #iterating over chunks\n    for i, chunk in enumerate(tokenized_inputs['input_ids']):\n        ids_of_tokens = tokenized_inputs.word_ids(i)\n        \n        org_word_ids_list.append(ids_of_tokens)\n        document_id.append(example['document'])\n        \n\n    tokenized_inputs['org_word_ids'] = org_word_ids_list\n    tokenized_inputs['document'] = document_id\n\n    return tokenized_inputs\n    \ndata_tokenized = dataset.map(tokenize_and_align, batched=False)\nprint(data_tokenized['document'][0])","metadata":{"execution":{"iopub.status.busy":"2024-04-22T19:06:28.192122Z","iopub.execute_input":"2024-04-22T19:06:28.192903Z","iopub.status.idle":"2024-04-22T19:06:28.363631Z","shell.execute_reply.started":"2024-04-22T19:06:28.192867Z","shell.execute_reply":"2024-04-22T19:06:28.362449Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1acc8d0c1f6c4d6a8cc2373139dfda53"}},"metadata":{}},{"name":"stdout","text":"[7, 7]\n","output_type":"stream"}]},{"cell_type":"code","source":"def flatten_data(data, keys_to_flatten):\n\n    data_flat = {}\n\n    for key in keys_to_flatten:\n        data_flat[key] = reduce(lambda x,y: x+y, data[key])\n\n    return Dataset.from_dict(data_flat)\n\nkeys_to_flatten = ['input_ids', 'token_type_ids', 'attention_mask', 'org_word_ids','document']\n\ndata_flat = flatten_data(data_tokenized, keys_to_flatten)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T19:07:09.236669Z","iopub.execute_input":"2024-04-22T19:07:09.237072Z","iopub.status.idle":"2024-04-22T19:07:09.287968Z","shell.execute_reply.started":"2024-04-22T19:07:09.237045Z","shell.execute_reply":"2024-04-22T19:07:09.286815Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"input_ids\ntoken_type_ids\nattention_mask\norg_word_ids\ndocument\n","output_type":"stream"}]},{"cell_type":"code","source":"print(len(data_flat))\n\ndata_tokenized['document']\ndata_flat.set_format(type='pt', columns=['input_ids', 'token_type_ids', 'attention_mask'])\nencoded = {'input_ids':data_tokenized['input_ids'], 'token_type_ids':data_tokenized['token_type_ids'], 'attention_mask':data_tokenized['attention_mask']}\noutputs = model(input_ids=data_flat['input_ids'], token_type_ids=data_flat['token_type_ids'], attention_mask=data_flat['attention_mask'], )\n","metadata":{"execution":{"iopub.status.busy":"2024-04-22T19:39:13.196547Z","iopub.execute_input":"2024-04-22T19:39:13.197073Z","iopub.status.idle":"2024-04-22T19:39:44.211809Z","shell.execute_reply.started":"2024-04-22T19:39:13.197030Z","shell.execute_reply":"2024-04-22T19:39:44.210659Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"21\n","output_type":"stream"}]},{"cell_type":"code","source":"data_flat","metadata":{"execution":{"iopub.status.busy":"2024-04-22T19:43:02.887739Z","iopub.execute_input":"2024-04-22T19:43:02.888366Z","iopub.status.idle":"2024-04-22T19:43:02.898288Z","shell.execute_reply.started":"2024-04-22T19:43:02.888317Z","shell.execute_reply":"2024-04-22T19:43:02.897065Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['input_ids', 'token_type_ids', 'attention_mask', 'org_word_ids', 'document'],\n    num_rows: 21\n})"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\nimport torch\npredictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\npredictions = predictions.detach().numpy()\ntest_preds = np.argmax(predictions, axis=-1)\nprint(len(test_preds))","metadata":{"execution":{"iopub.status.busy":"2024-04-22T19:21:42.759126Z","iopub.execute_input":"2024-04-22T19:21:42.760177Z","iopub.status.idle":"2024-04-22T19:21:42.766766Z","shell.execute_reply.started":"2024-04-22T19:21:42.760140Z","shell.execute_reply":"2024-04-22T19:21:42.765837Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"21\n","output_type":"stream"}]},{"cell_type":"code","source":"model.config.id2label","metadata":{"execution":{"iopub.status.busy":"2024-04-22T19:06:28.740577Z","iopub.status.idle":"2024-04-22T19:06:28.741107Z","shell.execute_reply.started":"2024-04-22T19:06:28.740840Z","shell.execute_reply":"2024-04-22T19:06:28.740862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"document_list = []\ntoken_id_list = []\nlabel_id_list = []\nfor doc, token_id, pred in zip(data_flat['document'],data_flat['org_word_ids'],test_preds):\n    for i in range(len(test_preds)):\n        current_word_id = token_id\n        if pred[i] != 14 and token_id[i] != None:\n            document_list.append(doc)\n            token_id_list.append(token_id[i])\n            label_id_list.append(pred[i])","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:14:51.353824Z","iopub.execute_input":"2024-04-22T20:14:51.354440Z","iopub.status.idle":"2024-04-22T20:14:51.369085Z","shell.execute_reply.started":"2024-04-22T20:14:51.354401Z","shell.execute_reply":"2024-04-22T20:14:51.367519Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"pred_df = pd.DataFrame(\n    {\n        \"document\": document_list,\n        \"token\": token_id_list,\n        \"label_id\": label_id_list,\n    }\n)\npred_df[\"label\"] = pred_df.label_id.map(model.config.id2label) # map integer label to BIO format label\nno_duplicates_df = pred_df.drop_duplicates(subset = ['document', 'token', 'label_id'],keep = 'first').reset_index(drop = True)\nfinal_df = no_duplicates_df.drop(columns=[\"label_id\"]) # remove extra columns\nfinal_df = final_df.rename_axis(\"row_id\").reset_index() # add `row_id` column\n# final_df.head(10)\nfinal_df.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-22T20:36:09.195816Z","iopub.execute_input":"2024-04-22T20:36:09.198107Z","iopub.status.idle":"2024-04-22T20:36:09.216133Z","shell.execute_reply.started":"2024-04-22T20:36:09.198064Z","shell.execute_reply":"2024-04-22T20:36:09.214969Z"},"trusted":true},"execution_count":51,"outputs":[]}]}