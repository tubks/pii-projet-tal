{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains the unfinished, work-in-progress methods. To access the working, current evolution of the baseline check out baseline.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf188be7029470782a36193e48b8d39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f0250671ba34f5dad6642b9c6c3a9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "654b04a6cd6c410191952cdaf26e6f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2735cb8c3842b19c20222db125cfd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce649e1488734425a246cf7d76940a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = 'roberta-base'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('..','data', 'raw', 'train.json')\n",
    "\n",
    "data = pd.read_json(data_path)\n",
    "\n",
    "data_dataset = Dataset.from_pandas(data)\n",
    "#data_dataset = data_dataset.remove_columns(['full_text','trailing_whitespace', 'document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer(data_dataset['full_text'][0], padding=True, truncation=True, return_overflowing_tokens=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaTokenizerFast(name_or_path='roberta-base', vocab_size=50265, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': '<mask>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "\t50264: AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 65,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 76,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 179,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 230,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 475,\n",
       " 475,\n",
       " 475,\n",
       " 476,\n",
       " 477,\n",
       " 478,\n",
       " 478,\n",
       " 479,\n",
       " None]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]ctions. this second workshop also lasts two hours and allows the mind map to evolve. once familiarized with it, the stakeholders discover the power of the tool. then, the second workshop brings out even more ideas and constructive exchanges between the stakeholders. around this new mind map, they have learned to work together and want to make visible the untold ideas. i now present all the projects i manage in this type of format in order to ease rapid understanding for decision - makers. these presentations are the core of my business models. the decision - makers are thus able to identify the opportunities of the projects and can take quick decisions to validate them. they find answers to their questions thank to a schematic representation. approach what i find amazing with the facilitation of this type of workshop is the participants commitment for the project. this tool helps to give meaning. the participants appropriate the story and want to keep writing it. then, they easily become actors or sponsors of the project. a trust relationship is built, thus facilitating the implementation of related actions. design thinking for innovation reflexion - avril 2021 - nathalie sylla annex 1 : mind map shared facilities project [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 58,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 444,\n",
       " 444,\n",
       " 445,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 474,\n",
       " 474,\n",
       " None]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized.word_ids(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-NAME_STUDENT',\n",
       " 1: 'B-EMAIL',\n",
       " 2: 'B-USERNAME',\n",
       " 3: 'B-ID_NUM',\n",
       " 4: 'B-PHONE_NUM',\n",
       " 5: 'B-URL_PERSONAL',\n",
       " 6: 'B-STREET_ADDRESS',\n",
       " 7: 'I-NAME_STUDENT',\n",
       " 8: 'I-EMAIL',\n",
       " 9: 'I-USERNAME',\n",
       " 10: 'I-ID_NUM',\n",
       " 11: 'I-PHONE_NUM',\n",
       " 12: 'I-URL_PERSONAL',\n",
       " 13: 'I-STREET_ADDRESS',\n",
       " 14: 'O',\n",
       " -100: '[PAD]'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS_LIST = ['B-NAME_STUDENT', 'B-EMAIL', 'B-USERNAME', 'B-ID_NUM', 'B-PHONE_NUM', 'B-URL_PERSONAL', 'B-STREET_ADDRESS', 'I-NAME_STUDENT', 'I-EMAIL', 'I-USERNAME', 'I-ID_NUM', 'I-PHONE_NUM','I-URL_PERSONAL','I-STREET_ADDRESS', 'O']\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(LABELS_LIST)}\n",
    "label2id['[PAD]'] = -100\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - version 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(example):\n",
    "    \"\"\"\n",
    "    to be used with datasets.map() with batched=False\n",
    "    \n",
    "    Encodes the labels into integers.\n",
    "    \n",
    "    \"\"\"\n",
    "    labels = example['labels']\n",
    "    encoded = [label2id[label] for label in labels]\n",
    "    return {'labels': encoded}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339b3a09f8134930a384bdb2d13bda9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_labels_encoded = data_dataset.map(encode_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'],\n",
       "    num_rows: 6807\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef0a381cd814c23b9286fd249581610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tokenize_and_align(example, overlap_size = 0):\n",
    "    \"\"\"\n",
    "    To be used with datasets.map() with batched=False\n",
    "\n",
    "    Takes in \n",
    "        - example : an example from the datasets class\n",
    "        - overlap_size: the number of tokens that overlap between two consecutive chunks\n",
    "        \n",
    "    outputs:\n",
    "        - a Dict[]->List with columns:\n",
    "            - of the bert tokenizer output\n",
    "            - encoded labels\n",
    "    \"\"\"\n",
    "\n",
    "    org_labels = example['labels']\n",
    "    tokenized_inputs = tokenizer(example['tokens'], is_split_into_words=True, return_offsets_mapping=True, truncation=True, padding='max_length', max_length=512, return_overflowing_tokens=True, stride=overlap_size, return_tensors='pt')\n",
    "    tokenized_inputs.pop('overflow_to_sample_mapping')\n",
    "    tokenized_inputs.pop('offset_mapping')\n",
    "    \n",
    "    new_labels = []\n",
    "    org_word_ids_list = []\n",
    "    document_id = []\n",
    "    #iterating over chunks\n",
    "    for i, chunk in enumerate(tokenized_inputs['input_ids']):\n",
    "        ids_of_tokens = tokenized_inputs.word_ids(i)\n",
    "        \n",
    "        org_word_ids_list.append(ids_of_tokens)\n",
    "        document_id.append(example['document'])\n",
    "        #iterating over ids of tokens\n",
    "        chunk_labels = []\n",
    "        for id in ids_of_tokens:\n",
    "            #if id=None, then it means it's some BERT token (CLS, SEP or PAD)\n",
    "            if id is None:\n",
    "                chunk_labels.append(-100)\n",
    "            else:\n",
    "                chunk_labels.append(org_labels[id])\n",
    "        new_labels.append(chunk_labels)\n",
    "\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    tokenized_inputs['org_word_ids'] = org_word_ids_list\n",
    "    tokenized_inputs['document'] = document_id\n",
    "\n",
    "    return tokenized_inputs\n",
    "    \n",
    "data_small = data_labels_encoded.select(range(1))\n",
    "data_small = data_small.map(tokenize_and_align, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932e90efe8c94c2e8efeaa2bdab60443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_encoded_all = data_labels_encoded.map(tokenize_and_align, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'new_labels'],\n",
       "    num_rows: 6807\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_encoded_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 7]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_encoded_all['document'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'org_word_ids', 'document'],\n",
       "    num_rows: 12812\n",
       "})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten_data(data, keys_to_flatten):\n",
    "\n",
    "    data_flat = {}\n",
    "\n",
    "    for key in keys_to_flatten:\n",
    "        data_flat[key] = reduce(lambda x,y: x+y, data[key])\n",
    "\n",
    "\n",
    "    return Dataset.from_dict(data_flat)\n",
    "\n",
    "keys_to_flatten = ['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'org_word_ids','document']\n",
    "\n",
    "\n",
    "data_flat = flatten_data(data_encoded_all, keys_to_flatten)\n",
    "\n",
    "data_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([25682, 1011, 14085, 8865, 2666], [14, 14, 0, 0, 0], '2021 - nathalie')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_small['input_ids'][0][0][10:15], data_small['new_labels'][0][0][10:15], tokenizer.decode(data_small['input_ids'][0][0][10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_small['input_ids'][0][1]), len(data_small['new_labels'][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_labels = reduce(lambda x,y: x+y, data_small['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chunked_encoded = data_labels_encoded.map(lambda x: tokenize_and_align(x,sub_word_labeling=True), batched=False)\n",
    "#time to tokenize and chunk WITH subword labeling : 1m58s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels aligned with tokens, with subword labeling\n",
      "['thinking', 'for', 'innovation', 'reflex', '##ion', '-', 'av', '##ril', '2021', '-', 'nat', '##hal']\n",
      "[14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"labels aligned with tokens, with subword labeling\")\n",
    "print(tokenizer.convert_ids_to_tokens(data_chunked_encoded['input_ids'][0][0][2:14]))\n",
    "print(data_chunked_encoded['labels'][0][0][2:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d5b7aaf1a142fb9236344d0f8a4294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_chunked_encoded_sub_labeling_false = data_labels_encoded.map(lambda x: tokenize_and_align(x,sub_word_labeling=False), batched=False)\n",
    "#time to tokenize and chunk WITHOUT subword labeling : 46s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels aligned with tokens, without subword labeling\n",
      "['thinking', 'for', 'innovation', 'reflex', '##ion', '-', 'av', '##ril', '2021', '-', 'nat', '##hal']\n",
      "[14, 14, 14, 14, -100, 14, 14, -100, 14, 14, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "print(\"labels aligned with tokens, without subword labeling\")\n",
    "print(tokenizer.convert_ids_to_tokens(data_chunked_encoded_sub_labeling_false['input_ids'][0][0][2:14]))\n",
    "print(data_chunked_encoded_sub_labeling_false['labels'][0][0][2:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#flatten the dataset\n",
    "data_flat = {}\n",
    "\n",
    "keys = ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "for key in keys:\n",
    "    data_flat[key] = reduce(lambda x,y: x+y, data_chunked_encoded[key])\n",
    "\n",
    "data_flat = Dataset.from_dict(data_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if each row has length 512 for each column\n",
      "All good\n"
     ]
    }
   ],
   "source": [
    "#checking if each row has length 512 for each column\n",
    "print(\"Checking if each row has length 512 for each column\")\n",
    "for example in data_flat:\n",
    "    for key in keys:\n",
    "        if len(example[key]) != 512:\n",
    "            print(f\"Error in {key}, length is {len(example[key])} instead of 512\")\n",
    "            break\n",
    "print(\"All good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_flat['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - version 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(examples):\n",
    "    labels = []\n",
    "    tokenized_sentence = []\n",
    "    for word, label in zip(examples['tokens'], examples['labels']):\n",
    "        #tokenizes the word using BERT's subword tokenizer\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        #adds the same label to all the subwords of the word\n",
    "        labels.extend([label] * n_subwords)\n",
    "    examples['tokens'] = tokenized_sentence\n",
    "    examples['labels'] = labels\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7388df55e2dc4368a24b1768fd957c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = data_dataset.map(tokenize_and_preserve_labels,batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(examples, block_size=510, sliding_window=False):\n",
    "    tokenized_sentences = []\n",
    "    labels = []\n",
    "    for i in range(0, len(examples['tokens']), block_size):\n",
    "        chunk_token = examples['tokens'][i:i+block_size]\n",
    "        chunk_label = examples['labels'][i:i+block_size]\n",
    "        if len(chunk_token) < block_size:\n",
    "            chunk_token += ['[PAD]'] * (block_size - len(chunk_token))\n",
    "            chunk_label += ['[PAD]'] * (block_size - len(chunk_label))\n",
    "        tokenized_sentences.append(chunk_token)\n",
    "        labels.append(chunk_label)\n",
    "    return {'tokens': tokenized_sentences, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d80b42c1c1435582b3f12c159b5f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunked_data = tokenized_data.map(chunk, batched=False)\n",
    "chunked_data = chunked_data.remove_columns(['document', 'full_text','trailing_whitespace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "flat_chunks_tokens = list(reduce(lambda x, y: x + y, chunked_data['tokens'], []))\n",
    "flat_chunks_labels = list(reduce(lambda x, y: x + y, chunked_data['labels'], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_flattened_data = Dataset.from_dict({'tokens': flat_chunks_tokens, 'labels': flat_chunks_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the '[PAD]' label to be encoded as -100 so that it's ignored by the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'labels'],\n",
       "    num_rows: 12812\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_flattened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(example):\n",
    "    detokenized = list(map(lambda x: ' '.join(x), example['tokens']))\n",
    "    detokenized = list(map(lambda x: x.replace(' ##', ''), detokenized))\n",
    "    encoded = tokenizer(detokenized, truncation=True, is_split_into_words = False, return_tensors='pt')\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80723d742884fa99d52d4c2756d7a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_data = chunked_flattened_data.map(encode_tokens, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d6681c5ffb42dfb5406c2944fd6234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def encode_labels(example):\n",
    "    labels = example['labels']\n",
    "    #adding -100 for the [CLS] token and [SEP] token\n",
    "    encoded = [-100] + [label2id[label] for label in labels] + [-100]\n",
    "    return {'labels': encoded}\n",
    "\n",
    "encoded_labels = encoded_data.map(encode_labels, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 12812\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(510, 512, 512)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_labels['tokens'][0]), len(encoded_labels['labels'][0]), len(encoded_labels['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_split = encoded_labels.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- nathalie sylla challenge & selection'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_labels['input_ids'][0][11:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 11530\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1282\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_data_split = data_flat.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [LABELS_LIST[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [LABELS_LIST[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=len(id2label), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing the BERT layers\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b14a647a5584fc498c7c895cd02fec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 4.4220989366620436e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0005, 'learning_rate': 3.844197873324087e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696205f23f7c42a09c560424aa5f5ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00029877974884584546, 'eval_precision': 0.9052132701421801, 'eval_recall': 0.9646464646464646, 'eval_f1': 0.9339853300733496, 'eval_accuracy': 0.9998955729182983, 'eval_runtime': 53.9588, 'eval_samples_per_second': 23.759, 'eval_steps_per_second': 2.984, 'epoch': 1.0}\n",
      "{'loss': 0.0006, 'learning_rate': 3.266296809986131e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0006, 'learning_rate': 2.688395746648174e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0004, 'learning_rate': 2.1104946833102173e-05, 'epoch': 1.73}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52de4190c4b6415eb7bcf1dc685993cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0002911491028498858, 'eval_precision': 0.9073634204275535, 'eval_recall': 0.9646464646464646, 'eval_f1': 0.9351285189718482, 'eval_accuracy': 0.9998976614599324, 'eval_runtime': 53.6274, 'eval_samples_per_second': 23.906, 'eval_steps_per_second': 3.002, 'epoch': 2.0}\n",
      "{'loss': 0.0005, 'learning_rate': 1.5325936199722607e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0005, 'learning_rate': 9.546925566343042e-06, 'epoch': 2.43}\n",
      "{'loss': 0.0005, 'learning_rate': 3.7679149329634766e-06, 'epoch': 2.77}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078ed1e2c91447d89a71a0cbcdb824aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0002923872380051762, 'eval_precision': 0.9073634204275535, 'eval_recall': 0.9646464646464646, 'eval_f1': 0.9351285189718482, 'eval_accuracy': 0.9998976614599324, 'eval_runtime': 53.2128, 'eval_samples_per_second': 24.092, 'eval_steps_per_second': 3.026, 'epoch': 3.0}\n",
      "{'train_runtime': 1946.9332, 'train_samples_per_second': 17.766, 'train_steps_per_second': 2.222, 'train_loss': 0.0004889280520210759, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4326, training_loss=0.0004889280520210759, metrics={'train_runtime': 1946.9332, 'train_samples_per_second': 17.766, 'train_steps_per_second': 2.222, 'train_loss': 0.0004889280520210759, 'epoch': 3.0})"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"model_baseline_v0.1\",\n",
    "#     learning_rate=2e-3,\n",
    "#     per_device_train_batch_size=20,\n",
    "#     per_device_eval_batch_size=20,\n",
    "#     num_train_epochs=2,\n",
    "#     weight_decay=0.01,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "# )\n",
    "\n",
    "\n",
    "target_dir = \"model/trainer_model_initial_preprocessing\"\n",
    "\n",
    "training_args = TrainingArguments(output_dir=target_dir, evaluation_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_data_split[\"train\"],\n",
    "    eval_dataset=encoded_data_split[\"test\"],\n",
    "    #tokenizer=tokenizer,\n",
    "    #data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('model/model_initial_preprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#put trainer on cpu\n",
    "\n",
    "trainer.model.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0017704bc38c41f9b501d3e844741b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.0002923872380051762,\n",
       " 'eval_precision': 0.9073634204275535,\n",
       " 'eval_recall': 0.9646464646464646,\n",
       " 'eval_f1': 0.9351285189718482,\n",
       " 'eval_accuracy': 0.9998976614599324,\n",
       " 'eval_runtime': 54.1181,\n",
       " 'eval_samples_per_second': 23.689,\n",
       " 'eval_steps_per_second': 2.975,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = 'my name is John Smith and my email is john.smith@gmail.com.'\n",
    "\n",
    "model = model.to('cpu')\n",
    "\n",
    "predictions = model(**tokenizer(sen, return_tensors='pt'))\n",
    "predictions = np.argmax(predictions.logits.detach().numpy(), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokens = tokenizer.tokenize(sen)\n",
    "bert_tokens = ['[CLS]'] + bert_tokens + ['[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14, 14, 14, 14,  0,  7, 14, 14, 14, 14,  0,  1,  1,  1,  1,  1,\n",
       "         1,  1, 14, 14]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', '[CLS]'),\n",
       " ('O', 'my'),\n",
       " ('O', 'name'),\n",
       " ('O', 'is'),\n",
       " ('B-NAME_STUDENT', 'john'),\n",
       " ('I-NAME_STUDENT', 'smith'),\n",
       " ('O', 'and'),\n",
       " ('O', 'my'),\n",
       " ('O', 'email'),\n",
       " ('O', 'is'),\n",
       " ('B-NAME_STUDENT', 'john'),\n",
       " ('B-EMAIL', '.'),\n",
       " ('B-EMAIL', 'smith'),\n",
       " ('B-EMAIL', '@'),\n",
       " ('B-EMAIL', 'gma'),\n",
       " ('B-EMAIL', '##il'),\n",
       " ('B-EMAIL', '.'),\n",
       " ('B-EMAIL', 'com'),\n",
       " ('O', '.'),\n",
       " ('O', '[SEP]')]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(id2label[pred], token) for pred, token in zip(predictions[0], bert_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model into a pipeline and evaluaye it on the test set\n",
    "\n",
    "from transformers import TokenClassificationPipeline\n",
    "\n",
    "model_loaded = AutoModelForTokenClassification.from_pretrained('model/model_initial_preprocessing')\n",
    "model_loaded = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', '[CLS]'),\n",
       " ('O', \"'\"),\n",
       " ('O', 'the'),\n",
       " ('O', 'consequences'),\n",
       " ('O', 'of'),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', \"'\"),\n",
       " ('O', ','),\n",
       " ('O', 'by'),\n",
       " ('B-NAME_STUDENT', 'john'),\n",
       " ('I-NAME_STUDENT', 'smith'),\n",
       " ('O', 'the'),\n",
       " ('O', 'consequences'),\n",
       " ('O', 'of'),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'are'),\n",
       " ('O', 'many'),\n",
       " ('O', '.'),\n",
       " ('O', 'one'),\n",
       " ('O', 'of'),\n",
       " ('O', 'the'),\n",
       " ('O', 'most'),\n",
       " ('O', 'important'),\n",
       " ('O', 'consequences'),\n",
       " ('O', 'is'),\n",
       " ('O', 'that'),\n",
       " ('O', 'it'),\n",
       " ('O', 'allows'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'achieve'),\n",
       " ('O', 'our'),\n",
       " ('O', 'goals'),\n",
       " ('O', '.'),\n",
       " ('O', 'when'),\n",
       " ('O', 'we'),\n",
       " ('O', 'plan'),\n",
       " ('O', ','),\n",
       " ('O', 'we'),\n",
       " ('O', 'are'),\n",
       " ('O', 'able'),\n",
       " ('O', 'to'),\n",
       " ('O', 'think'),\n",
       " ('O', 'about'),\n",
       " ('O', 'what'),\n",
       " ('O', 'we'),\n",
       " ('O', 'want'),\n",
       " ('O', 'to'),\n",
       " ('O', 'achieve'),\n",
       " ('O', 'and'),\n",
       " ('O', 'how'),\n",
       " ('O', 'we'),\n",
       " ('O', 'are'),\n",
       " ('O', 'going'),\n",
       " ('O', 'to'),\n",
       " ('O', 'achieve'),\n",
       " ('O', 'it'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'helps'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'focus'),\n",
       " ('O', 'on'),\n",
       " ('O', 'what'),\n",
       " ('O', 'is'),\n",
       " ('O', 'important'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'avoid'),\n",
       " ('O', 'wasting'),\n",
       " ('O', 'time'),\n",
       " ('O', 'on'),\n",
       " ('O', 'things'),\n",
       " ('O', 'that'),\n",
       " ('O', 'are'),\n",
       " ('O', 'not'),\n",
       " ('O', 'important'),\n",
       " ('O', '.'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'also'),\n",
       " ('O', 'helps'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'see'),\n",
       " ('O', 'the'),\n",
       " ('O', 'big'),\n",
       " ('O', 'picture'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'understand'),\n",
       " ('O', 'how'),\n",
       " ('O', 'all'),\n",
       " ('O', 'the'),\n",
       " ('O', 'different'),\n",
       " ('O', 'parts'),\n",
       " ('O', 'of'),\n",
       " ('O', 'a'),\n",
       " ('O', 'project'),\n",
       " ('O', 'fit'),\n",
       " ('O', 'together'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'make'),\n",
       " ('O', 'better'),\n",
       " ('O', 'decisions'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'avoid'),\n",
       " ('O', 'making'),\n",
       " ('O', 'mistakes'),\n",
       " ('O', '.'),\n",
       " ('O', 'another'),\n",
       " ('O', 'consequence'),\n",
       " ('O', 'of'),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'is'),\n",
       " ('O', 'that'),\n",
       " ('O', 'it'),\n",
       " ('O', 'helps'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'be'),\n",
       " ('O', 'more'),\n",
       " ('O', 'efficient'),\n",
       " ('O', '.'),\n",
       " ('O', 'when'),\n",
       " ('O', 'we'),\n",
       " ('O', 'plan'),\n",
       " ('O', ','),\n",
       " ('O', 'we'),\n",
       " ('O', 'can'),\n",
       " ('O', 'break'),\n",
       " ('O', 'a'),\n",
       " ('O', 'big'),\n",
       " ('O', 'task'),\n",
       " ('O', 'down'),\n",
       " ('O', 'into'),\n",
       " ('O', 'smaller'),\n",
       " ('O', 'tasks'),\n",
       " ('O', 'and'),\n",
       " ('O', 'then'),\n",
       " ('O', 'work'),\n",
       " ('O', 'on'),\n",
       " ('O', 'each'),\n",
       " ('O', 'task'),\n",
       " ('O', 'one'),\n",
       " ('O', 'at'),\n",
       " ('O', 'a'),\n",
       " ('O', 'time'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'stay'),\n",
       " ('O', 'focused'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'avoid'),\n",
       " ('O', 'feeling'),\n",
       " ('O', 'overwhelmed'),\n",
       " ('O', '.'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'can'),\n",
       " ('O', 'also'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'identify'),\n",
       " ('O', 'potential'),\n",
       " ('O', 'problems'),\n",
       " ('O', 'before'),\n",
       " ('O', 'they'),\n",
       " ('O', 'occur'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'come'),\n",
       " ('O', 'up'),\n",
       " ('O', 'with'),\n",
       " ('O', 'solutions'),\n",
       " ('O', 'to'),\n",
       " ('O', 'those'),\n",
       " ('O', 'problems'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'save'),\n",
       " ('O', 'time'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'avoid'),\n",
       " ('O', 'wasting'),\n",
       " ('O', 'resources'),\n",
       " ('O', '.'),\n",
       " ('O', 'finally'),\n",
       " ('O', ','),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'be'),\n",
       " ('O', 'more'),\n",
       " ('O', 'successful'),\n",
       " ('O', '.'),\n",
       " ('O', 'when'),\n",
       " ('O', 'we'),\n",
       " ('O', 'plan'),\n",
       " ('O', ','),\n",
       " ('O', 'we'),\n",
       " ('O', 'are'),\n",
       " ('O', 'able'),\n",
       " ('O', 'to'),\n",
       " ('O', 'set'),\n",
       " ('O', 'clear'),\n",
       " ('O', 'goals'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'create'),\n",
       " ('O', 'a'),\n",
       " ('O', 'road'),\n",
       " ('O', '##ma'),\n",
       " ('O', '##p'),\n",
       " ('O', 'for'),\n",
       " ('O', 'how'),\n",
       " ('O', 'we'),\n",
       " ('O', 'are'),\n",
       " ('O', 'going'),\n",
       " ('O', 'to'),\n",
       " ('O', 'achieve'),\n",
       " ('O', 'those'),\n",
       " ('O', 'goals'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'stay'),\n",
       " ('O', 'motivated'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'keep'),\n",
       " ('O', 'moving'),\n",
       " ('O', 'forward'),\n",
       " ('O', ','),\n",
       " ('O', 'even'),\n",
       " ('O', 'when'),\n",
       " ('O', 'things'),\n",
       " ('O', 'get'),\n",
       " ('O', 'tough'),\n",
       " ('O', '.'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'can'),\n",
       " ('O', 'also'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'track'),\n",
       " ('O', 'our'),\n",
       " ('O', 'progress'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'see'),\n",
       " ('O', 'how'),\n",
       " ('O', 'far'),\n",
       " ('O', 'we'),\n",
       " ('O', 'have'),\n",
       " ('O', 'come'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'stay'),\n",
       " ('O', 'focused'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'keep'),\n",
       " ('O', 'working'),\n",
       " ('O', 'towards'),\n",
       " ('O', 'our'),\n",
       " ('O', 'goals'),\n",
       " ('O', ','),\n",
       " ('O', 'even'),\n",
       " ('O', 'when'),\n",
       " ('O', 'we'),\n",
       " ('O', 'face'),\n",
       " ('O', 'obstacles'),\n",
       " ('O', '.'),\n",
       " ('O', 'in'),\n",
       " ('O', 'conclusion'),\n",
       " ('O', ','),\n",
       " ('O', 'the'),\n",
       " ('O', 'consequences'),\n",
       " ('O', 'of'),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'are'),\n",
       " ('O', 'many'),\n",
       " ('O', '.'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'achieve'),\n",
       " ('O', 'our'),\n",
       " ('O', 'goals'),\n",
       " ('O', ','),\n",
       " ('O', 'to'),\n",
       " ('O', 'be'),\n",
       " ('O', 'more'),\n",
       " ('O', 'efficient'),\n",
       " ('O', ','),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'be'),\n",
       " ('O', 'more'),\n",
       " ('O', 'successful'),\n",
       " ('O', '.'),\n",
       " ('O', 'it'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'focus'),\n",
       " ('O', 'on'),\n",
       " ('O', 'what'),\n",
       " ('O', 'is'),\n",
       " ('O', 'important'),\n",
       " ('O', ','),\n",
       " ('O', 'to'),\n",
       " ('O', 'avoid'),\n",
       " ('O', 'wasting'),\n",
       " ('O', 'time'),\n",
       " ('O', ','),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'make'),\n",
       " ('O', 'better'),\n",
       " ('O', 'decisions'),\n",
       " ('O', '.'),\n",
       " ('O', 'it'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'stay'),\n",
       " ('O', 'motivated'),\n",
       " ('O', ','),\n",
       " ('O', 'to'),\n",
       " ('O', 'track'),\n",
       " ('O', 'our'),\n",
       " ('O', 'progress'),\n",
       " ('O', ','),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'keep'),\n",
       " ('O', 'moving'),\n",
       " ('O', 'forward'),\n",
       " ('O', '.'),\n",
       " ('O', 'in'),\n",
       " ('O', 'short'),\n",
       " ('O', ','),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'is'),\n",
       " ('O', 'essential'),\n",
       " ('O', 'for'),\n",
       " ('O', 'success'),\n",
       " ('O', 'in'),\n",
       " ('O', 'any'),\n",
       " ('O', 'endeavor'),\n",
       " ('O', '.'),\n",
       " ('O', 'email'),\n",
       " ('O', ':'),\n",
       " ('B-EMAIL', 'john'),\n",
       " ('B-EMAIL', '.'),\n",
       " ('B-EMAIL', 'smith'),\n",
       " ('B-EMAIL', '@'),\n",
       " ('B-EMAIL', 'b'),\n",
       " ('B-EMAIL', '##mail'),\n",
       " ('B-EMAIL', '.'),\n",
       " ('B-EMAIL', 'com'),\n",
       " ('O', 'phone'),\n",
       " ('O', ':'),\n",
       " ('B-EMAIL', '123'),\n",
       " ('I-PHONE_NUM', '-'),\n",
       " ('B-EMAIL', '45'),\n",
       " ('B-EMAIL', '##6'),\n",
       " ('I-PHONE_NUM', '-'),\n",
       " ('B-EMAIL', '78'),\n",
       " ('B-EMAIL', '##90'),\n",
       " ('O', '[SEP]')]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay = \"\"\"\n",
    "'The consequences of Great Planning', by John Smith\n",
    "\n",
    "The consequences of great planning are many. One of the most important consequences is that it allows us to achieve our goals. When we plan, we are able to think about what we want to achieve and how we are going to achieve it. This helps us to focus on what is important and to avoid wasting time on things that are not important. Planning also helps us to see the big picture and to understand how all the different parts of a project fit together. This can help us to make better decisions and to avoid making mistakes.\n",
    "\n",
    "Another consequence of great planning is that it helps us to be more efficient. When we plan, we can break a big task down into smaller tasks and then work on each task one at a time. This can help us to stay focused and to avoid feeling overwhelmed. Planning can also help us to identify potential problems before they occur and to come up with solutions to those problems. This can help us to save time and to avoid wasting resources.\n",
    "\n",
    "Finally, great planning can help us to be more successful. When we plan, we are able to set clear goals and to create a roadmap for how we are going to achieve those goals. This can help us to stay motivated and to keep moving forward, even when things get tough. Planning can also help us to track our progress and to see how far we have come. This can help us to stay focused and to keep working towards our goals, even when we face obstacles.\n",
    "\n",
    "In conclusion, the consequences of great planning are many. Planning can help us to achieve our goals, to be more efficient, and to be more successful. It can help us to focus on what is important, to avoid wasting time, and to make better decisions. It can help us to stay motivated, to track our progress, and to keep moving forward. In short, great planning is essential for success in any endeavor.\n",
    "\n",
    "email: john.smith@bmail.com\n",
    "phone: 123-456-7890\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "predictions = model(**tokenizer(essay, return_tensors='pt'))\n",
    "predictions = np.argmax(predictions.logits.detach().numpy(), axis=2)\n",
    "\n",
    "bert_tokens = tokenizer.tokenize(essay)\n",
    "bert_tokens = ['[CLS]'] + bert_tokens + ['[SEP]']\n",
    "\n",
    "[(id2label[pred], token) for pred, token in zip(predictions[0], bert_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2048b70fc4d4074b1551f2400baa060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.010430662892758846,\n",
       " 'eval_precision': 0.10179640718562874,\n",
       " 'eval_recall': 0.22869955156950672,\n",
       " 'eval_f1': 0.1408839779005525,\n",
       " 'eval_accuracy': 0.9983660777738643,\n",
       " 'eval_runtime': 56.1491,\n",
       " 'eval_samples_per_second': 22.832,\n",
       " 'eval_steps_per_second': 2.867}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_data_split[\"train\"],\n",
    "    eval_dataset=encoded_data_split[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " -100]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_flat['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_labels['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474 14 0\n",
      "475 14 0\n",
      "476 14 0\n",
      "477 14 7\n",
      "478 14 7\n",
      "['nat', '##hal', '##ie', 'sy', '##lla']\n",
      "['nat', '##hal', '##ie', 'sy', '##lla']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, (l1,l2) in enumerate(zip(data_flat['labels'][0], encoded_labels['labels'][0])):\n",
    "    if l1 != l2:\n",
    "        print(i, l1, l2)\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(encoded_labels['input_ids'][0][474:479]))\n",
    "print(tokenizer.convert_ids_to_tokens(data_flat['input_ids'][0][474:479]))\n",
    "\n",
    "data_flat['input_ids'][0] == encoded_labels['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_flat['labels'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
