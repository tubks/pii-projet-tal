{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains the unfinished, work-in-progress methods. To access the working, current evolution of the baseline check out baseline.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('..','data', 'raw', 'train.json')\n",
    "\n",
    "data = pd.read_json(data_path)\n",
    "\n",
    "data_dataset = Dataset.from_pandas(data)\n",
    "#data_dataset = data_dataset.remove_columns(['full_text','trailing_whitespace', 'document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer(data_dataset['full_text'], padding=True, truncation=True, return_overflowing_tokens=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]ctions. this second workshop also lasts two hours and allows the mind map to evolve. once familiarized with it, the stakeholders discover the power of the tool. then, the second workshop brings out even more ideas and constructive exchanges between the stakeholders. around this new mind map, they have learned to work together and want to make visible the untold ideas. i now present all the projects i manage in this type of format in order to ease rapid understanding for decision - makers. these presentations are the core of my business models. the decision - makers are thus able to identify the opportunities of the projects and can take quick decisions to validate them. they find answers to their questions thank to a schematic representation. approach what i find amazing with the facilitation of this type of workshop is the participants commitment for the project. this tool helps to give meaning. the participants appropriate the story and want to keep writing it. then, they easily become actors or sponsors of the project. a trust relationship is built, thus facilitating the implementation of related actions. design thinking for innovation reflexion - avril 2021 - nathalie sylla annex 1 : mind map shared facilities project [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized.word_ids(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-NAME_STUDENT',\n",
       " 1: 'B-EMAIL',\n",
       " 2: 'B-USERNAME',\n",
       " 3: 'B-ID_NUM',\n",
       " 4: 'B-PHONE_NUM',\n",
       " 5: 'B-URL_PERSONAL',\n",
       " 6: 'B-STREET_ADDRESS',\n",
       " 7: 'I-NAME_STUDENT',\n",
       " 8: 'I-EMAIL',\n",
       " 9: 'I-USERNAME',\n",
       " 10: 'I-ID_NUM',\n",
       " 11: 'I-PHONE_NUM',\n",
       " 12: 'I-URL_PERSONAL',\n",
       " 13: 'I-STREET_ADDRESS',\n",
       " 14: 'O',\n",
       " -100: '[PAD]'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS_LIST = ['B-NAME_STUDENT', 'B-EMAIL', 'B-USERNAME', 'B-ID_NUM', 'B-PHONE_NUM', 'B-URL_PERSONAL', 'B-STREET_ADDRESS', 'I-NAME_STUDENT', 'I-EMAIL', 'I-USERNAME', 'I-ID_NUM', 'I-PHONE_NUM','I-URL_PERSONAL','I-STREET_ADDRESS', 'O']\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(LABELS_LIST)}\n",
    "label2id['[PAD]'] = -100\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - version 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(example):\n",
    "    \"\"\"\n",
    "    to be used with datasets.map() with batched=False\n",
    "    \n",
    "    Encodes the labels into integers.\n",
    "    \n",
    "    \"\"\"\n",
    "    labels = example['labels']\n",
    "    encoded = [label2id[label] for label in labels]\n",
    "    return {'labels': encoded}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e6419af9daa4d4ebf73b86d1207d3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_labels_encoded = data_dataset.map(encode_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'],\n",
       "    num_rows: 6807\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "keyword argument repeated: max_length (815271139.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[19], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    tokenized_inputs = tokenizer(example['tokens'], is_split_into_words=True, return_offsets_mapping=True, max_length=tokenizer.model_max_length, truncation=True, padding='max_length', max_length=512, return_overflowing_tokens=True, stride=0, return_tensors='pt')\u001b[0m\n\u001b[0m                                                                                                                                                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m keyword argument repeated: max_length\n"
     ]
    }
   ],
   "source": [
    "example = data_labels_encoded[0]\n",
    "tokenized_inputs = tokenizer(example['tokens'], is_split_into_words=True, return_offsets_mapping=True, truncation=True, padding='max_length', max_length=512, return_overflowing_tokens=True, stride=0, return_tensors='pt')\n",
    "tokenized_inputs['input_ids'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03f63d97b0d84eecb5cdb8faa51b39d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_and_align(example, overlap_size = 0):\n",
    "    \"\"\"\n",
    "    To be used with datasets.map() with batched=False\n",
    "\n",
    "    Takes in \n",
    "        - example : an example from the datasets class\n",
    "        - overlap_size: the number of tokens that overlap between two consecutive chunks\n",
    "        \n",
    "    outputs:\n",
    "        - a Dict[]->List with columns:\n",
    "            - of the bert tokenizer output\n",
    "            - encoded labels\n",
    "    \"\"\"\n",
    "\n",
    "    org_labels = example['labels']\n",
    "    tokenized_inputs = tokenizer(example['tokens'], is_split_into_words=True, return_offsets_mapping=True, truncation=True, padding='max_length', max_length=512, return_overflowing_tokens=True, stride=overlap_size, return_tensors='pt')\n",
    "    tokenized_inputs.pop('overflow_to_sample_mapping')\n",
    "    tokenized_inputs.pop('offset_mapping')\n",
    "    \n",
    "    new_labels = []\n",
    "    org_word_ids_list = []\n",
    "    document_id = []\n",
    "    \n",
    "    #iterating over chunks\n",
    "    for i, chunk in enumerate(tokenized_inputs['input_ids']):\n",
    "        ids_of_tokens = tokenized_inputs.word_ids(i)\n",
    "        \n",
    "        org_word_ids_list.append(ids_of_tokens)\n",
    "        document_id.append(example['document'])\n",
    "        #iterating over ids of tokens\n",
    "        chunk_labels = []\n",
    "        for id in ids_of_tokens:\n",
    "            #if id=None, then it means it's some BERT token (CLS, SEP or PAD)\n",
    "            if id is None:\n",
    "                chunk_labels.append(-100)\n",
    "            else:\n",
    "                chunk_labels.append(org_labels[id])\n",
    "        new_labels.append(chunk_labels)\n",
    "\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    tokenized_inputs['org_word_ids'] = org_word_ids_list\n",
    "    tokenized_inputs['document'] = document_id\n",
    "\n",
    "    return tokenized_inputs\n",
    "    \n",
    "data_small = data_labels_encoded.select(range(1))\n",
    "data_small = data_small.map(tokenize_and_align, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932e90efe8c94c2e8efeaa2bdab60443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_encoded_all = data_labels_encoded.map(tokenize_and_align, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'new_labels'],\n",
       "    num_rows: 6807\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_encoded_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 7]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_encoded_all['document'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'org_word_ids', 'document'],\n",
       "    num_rows: 12812\n",
       "})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten_data(data, keys_to_flatten):\n",
    "\n",
    "    data_flat = {}\n",
    "\n",
    "    for key in keys_to_flatten:\n",
    "        data_flat[key] = reduce(lambda x,y: x+y, data[key])\n",
    "\n",
    "\n",
    "    return Dataset.from_dict(data_flat)\n",
    "\n",
    "keys_to_flatten = ['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'org_word_ids','document']\n",
    "\n",
    "\n",
    "data_flat = flatten_data(data_encoded_all, keys_to_flatten)\n",
    "\n",
    "data_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([25682, 1011, 14085, 8865, 2666], [14, 14, 0, 0, 0], '2021 - nathalie')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_small['input_ids'][0][0][10:15], data_small['new_labels'][0][0][10:15], tokenizer.decode(data_small['input_ids'][0][0][10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_small['input_ids'][0][1]), len(data_small['new_labels'][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_labels = reduce(lambda x,y: x+y, data_small['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chunked_encoded = data_labels_encoded.map(lambda x: tokenize_and_align(x,sub_word_labeling=True), batched=False)\n",
    "#time to tokenize and chunk WITH subword labeling : 1m58s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels aligned with tokens, with subword labeling\n",
      "['thinking', 'for', 'innovation', 'reflex', '##ion', '-', 'av', '##ril', '2021', '-', 'nat', '##hal']\n",
      "[14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"labels aligned with tokens, with subword labeling\")\n",
    "print(tokenizer.convert_ids_to_tokens(data_chunked_encoded['input_ids'][0][0][2:14]))\n",
    "print(data_chunked_encoded['labels'][0][0][2:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d5b7aaf1a142fb9236344d0f8a4294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_chunked_encoded_sub_labeling_false = data_labels_encoded.map(lambda x: tokenize_and_align(x,sub_word_labeling=False), batched=False)\n",
    "#time to tokenize and chunk WITHOUT subword labeling : 46s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels aligned with tokens, without subword labeling\n",
      "['thinking', 'for', 'innovation', 'reflex', '##ion', '-', 'av', '##ril', '2021', '-', 'nat', '##hal']\n",
      "[14, 14, 14, 14, -100, 14, 14, -100, 14, 14, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "print(\"labels aligned with tokens, without subword labeling\")\n",
    "print(tokenizer.convert_ids_to_tokens(data_chunked_encoded_sub_labeling_false['input_ids'][0][0][2:14]))\n",
    "print(data_chunked_encoded_sub_labeling_false['labels'][0][0][2:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#flatten the dataset\n",
    "data_flat = {}\n",
    "\n",
    "keys = ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "for key in keys:\n",
    "    data_flat[key] = reduce(lambda x,y: x+y, data_chunked_encoded[key])\n",
    "\n",
    "data_flat = Dataset.from_dict(data_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if each row has length 512 for each column\n",
      "All good\n"
     ]
    }
   ],
   "source": [
    "#checking if each row has length 512 for each column\n",
    "print(\"Checking if each row has length 512 for each column\")\n",
    "for example in data_flat:\n",
    "    for key in keys:\n",
    "        if len(example[key]) != 512:\n",
    "            print(f\"Error in {key}, length is {len(example[key])} instead of 512\")\n",
    "            break\n",
    "print(\"All good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_flat['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - version 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(examples):\n",
    "    labels = []\n",
    "    tokenized_sentence = []\n",
    "    for word, label in zip(examples['tokens'], examples['labels']):\n",
    "        #tokenizes the word using BERT's subword tokenizer\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        #adds the same label to all the subwords of the word\n",
    "        labels.extend([label] * n_subwords)\n",
    "    examples['tokens'] = tokenized_sentence\n",
    "    examples['labels'] = labels\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7388df55e2dc4368a24b1768fd957c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = data_dataset.map(tokenize_and_preserve_labels,batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(examples, block_size=510, sliding_window=False):\n",
    "    tokenized_sentences = []\n",
    "    labels = []\n",
    "    for i in range(0, len(examples['tokens']), block_size):\n",
    "        chunk_token = examples['tokens'][i:i+block_size]\n",
    "        chunk_label = examples['labels'][i:i+block_size]\n",
    "        if len(chunk_token) < block_size:\n",
    "            chunk_token += ['[PAD]'] * (block_size - len(chunk_token))\n",
    "            chunk_label += ['[PAD]'] * (block_size - len(chunk_label))\n",
    "        tokenized_sentences.append(chunk_token)\n",
    "        labels.append(chunk_label)\n",
    "    return {'tokens': tokenized_sentences, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d80b42c1c1435582b3f12c159b5f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunked_data = tokenized_data.map(chunk, batched=False)\n",
    "chunked_data = chunked_data.remove_columns(['document', 'full_text','trailing_whitespace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "flat_chunks_tokens = list(reduce(lambda x, y: x + y, chunked_data['tokens'], []))\n",
    "flat_chunks_labels = list(reduce(lambda x, y: x + y, chunked_data['labels'], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_flattened_data = Dataset.from_dict({'tokens': flat_chunks_tokens, 'labels': flat_chunks_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the '[PAD]' label to be encoded as -100 so that it's ignored by the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'labels'],\n",
       "    num_rows: 12812\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_flattened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(example):\n",
    "    detokenized = list(map(lambda x: ' '.join(x), example['tokens']))\n",
    "    detokenized = list(map(lambda x: x.replace(' ##', ''), detokenized))\n",
    "    encoded = tokenizer(detokenized, truncation=True, is_split_into_words = False, return_tensors='pt')\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80723d742884fa99d52d4c2756d7a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_data = chunked_flattened_data.map(encode_tokens, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d6681c5ffb42dfb5406c2944fd6234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def encode_labels(example):\n",
    "    labels = example['labels']\n",
    "    #adding -100 for the [CLS] token and [SEP] token\n",
    "    encoded = [-100] + [label2id[label] for label in labels] + [-100]\n",
    "    return {'labels': encoded}\n",
    "\n",
    "encoded_labels = encoded_data.map(encode_labels, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 12812\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(510, 512, 512)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_labels['tokens'][0]), len(encoded_labels['labels'][0]), len(encoded_labels['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_split = encoded_labels.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- nathalie sylla challenge & selection'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_labels['input_ids'][0][11:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 11530\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1282\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_data_split = data_flat.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [LABELS_LIST[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [LABELS_LIST[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=len(id2label), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing the BERT layers\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b14a647a5584fc498c7c895cd02fec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 4.4220989366620436e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0005, 'learning_rate': 3.844197873324087e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696205f23f7c42a09c560424aa5f5ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00029877974884584546, 'eval_precision': 0.9052132701421801, 'eval_recall': 0.9646464646464646, 'eval_f1': 0.9339853300733496, 'eval_accuracy': 0.9998955729182983, 'eval_runtime': 53.9588, 'eval_samples_per_second': 23.759, 'eval_steps_per_second': 2.984, 'epoch': 1.0}\n",
      "{'loss': 0.0006, 'learning_rate': 3.266296809986131e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0006, 'learning_rate': 2.688395746648174e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0004, 'learning_rate': 2.1104946833102173e-05, 'epoch': 1.73}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52de4190c4b6415eb7bcf1dc685993cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0002911491028498858, 'eval_precision': 0.9073634204275535, 'eval_recall': 0.9646464646464646, 'eval_f1': 0.9351285189718482, 'eval_accuracy': 0.9998976614599324, 'eval_runtime': 53.6274, 'eval_samples_per_second': 23.906, 'eval_steps_per_second': 3.002, 'epoch': 2.0}\n",
      "{'loss': 0.0005, 'learning_rate': 1.5325936199722607e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0005, 'learning_rate': 9.546925566343042e-06, 'epoch': 2.43}\n",
      "{'loss': 0.0005, 'learning_rate': 3.7679149329634766e-06, 'epoch': 2.77}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078ed1e2c91447d89a71a0cbcdb824aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0002923872380051762, 'eval_precision': 0.9073634204275535, 'eval_recall': 0.9646464646464646, 'eval_f1': 0.9351285189718482, 'eval_accuracy': 0.9998976614599324, 'eval_runtime': 53.2128, 'eval_samples_per_second': 24.092, 'eval_steps_per_second': 3.026, 'epoch': 3.0}\n",
      "{'train_runtime': 1946.9332, 'train_samples_per_second': 17.766, 'train_steps_per_second': 2.222, 'train_loss': 0.0004889280520210759, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4326, training_loss=0.0004889280520210759, metrics={'train_runtime': 1946.9332, 'train_samples_per_second': 17.766, 'train_steps_per_second': 2.222, 'train_loss': 0.0004889280520210759, 'epoch': 3.0})"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"model_baseline_v0.1\",\n",
    "#     learning_rate=2e-3,\n",
    "#     per_device_train_batch_size=20,\n",
    "#     per_device_eval_batch_size=20,\n",
    "#     num_train_epochs=2,\n",
    "#     weight_decay=0.01,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "# )\n",
    "\n",
    "\n",
    "target_dir = \"model/trainer_model_initial_preprocessing\"\n",
    "\n",
    "training_args = TrainingArguments(output_dir=target_dir, evaluation_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_data_split[\"train\"],\n",
    "    eval_dataset=encoded_data_split[\"test\"],\n",
    "    #tokenizer=tokenizer,\n",
    "    #data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('model/model_initial_preprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#put trainer on cpu\n",
    "\n",
    "trainer.model.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0017704bc38c41f9b501d3e844741b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.0002923872380051762,\n",
       " 'eval_precision': 0.9073634204275535,\n",
       " 'eval_recall': 0.9646464646464646,\n",
       " 'eval_f1': 0.9351285189718482,\n",
       " 'eval_accuracy': 0.9998976614599324,\n",
       " 'eval_runtime': 54.1181,\n",
       " 'eval_samples_per_second': 23.689,\n",
       " 'eval_steps_per_second': 2.975,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = 'my name is John Smith and my email is john.smith@gmail.com.'\n",
    "\n",
    "model = model.to('cpu')\n",
    "\n",
    "predictions = model(**tokenizer(sen, return_tensors='pt'))\n",
    "predictions = np.argmax(predictions.logits.detach().numpy(), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokens = tokenizer.tokenize(sen)\n",
    "bert_tokens = ['[CLS]'] + bert_tokens + ['[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14, 14, 14, 14,  0,  7, 14, 14, 14, 14,  0,  1,  1,  1,  1,  1,\n",
       "         1,  1, 14, 14]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', '[CLS]'),\n",
       " ('O', 'my'),\n",
       " ('O', 'name'),\n",
       " ('O', 'is'),\n",
       " ('B-NAME_STUDENT', 'john'),\n",
       " ('I-NAME_STUDENT', 'smith'),\n",
       " ('O', 'and'),\n",
       " ('O', 'my'),\n",
       " ('O', 'email'),\n",
       " ('O', 'is'),\n",
       " ('B-NAME_STUDENT', 'john'),\n",
       " ('B-EMAIL', '.'),\n",
       " ('B-EMAIL', 'smith'),\n",
       " ('B-EMAIL', '@'),\n",
       " ('B-EMAIL', 'gma'),\n",
       " ('B-EMAIL', '##il'),\n",
       " ('B-EMAIL', '.'),\n",
       " ('B-EMAIL', 'com'),\n",
       " ('O', '.'),\n",
       " ('O', '[SEP]')]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(id2label[pred], token) for pred, token in zip(predictions[0], bert_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model into a pipeline and evaluaye it on the test set\n",
    "\n",
    "from transformers import TokenClassificationPipeline\n",
    "\n",
    "model_loaded = AutoModelForTokenClassification.from_pretrained('model/model_initial_preprocessing')\n",
    "model_loaded = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', '[CLS]'),\n",
       " ('O', \"'\"),\n",
       " ('O', 'the'),\n",
       " ('O', 'consequences'),\n",
       " ('O', 'of'),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', \"'\"),\n",
       " ('O', ','),\n",
       " ('O', 'by'),\n",
       " ('B-NAME_STUDENT', 'john'),\n",
       " ('I-NAME_STUDENT', 'smith'),\n",
       " ('O', 'the'),\n",
       " ('O', 'consequences'),\n",
       " ('O', 'of'),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'are'),\n",
       " ('O', 'many'),\n",
       " ('O', '.'),\n",
       " ('O', 'one'),\n",
       " ('O', 'of'),\n",
       " ('O', 'the'),\n",
       " ('O', 'most'),\n",
       " ('O', 'important'),\n",
       " ('O', 'consequences'),\n",
       " ('O', 'is'),\n",
       " ('O', 'that'),\n",
       " ('O', 'it'),\n",
       " ('O', 'allows'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'achieve'),\n",
       " ('O', 'our'),\n",
       " ('O', 'goals'),\n",
       " ('O', '.'),\n",
       " ('O', 'when'),\n",
       " ('O', 'we'),\n",
       " ('O', 'plan'),\n",
       " ('O', ','),\n",
       " ('O', 'we'),\n",
       " ('O', 'are'),\n",
       " ('O', 'able'),\n",
       " ('O', 'to'),\n",
       " ('O', 'think'),\n",
       " ('O', 'about'),\n",
       " ('O', 'what'),\n",
       " ('O', 'we'),\n",
       " ('O', 'want'),\n",
       " ('O', 'to'),\n",
       " ('O', 'achieve'),\n",
       " ('O', 'and'),\n",
       " ('O', 'how'),\n",
       " ('O', 'we'),\n",
       " ('O', 'are'),\n",
       " ('O', 'going'),\n",
       " ('O', 'to'),\n",
       " ('O', 'achieve'),\n",
       " ('O', 'it'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'helps'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'focus'),\n",
       " ('O', 'on'),\n",
       " ('O', 'what'),\n",
       " ('O', 'is'),\n",
       " ('O', 'important'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'avoid'),\n",
       " ('O', 'wasting'),\n",
       " ('O', 'time'),\n",
       " ('O', 'on'),\n",
       " ('O', 'things'),\n",
       " ('O', 'that'),\n",
       " ('O', 'are'),\n",
       " ('O', 'not'),\n",
       " ('O', 'important'),\n",
       " ('O', '.'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'also'),\n",
       " ('O', 'helps'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'see'),\n",
       " ('O', 'the'),\n",
       " ('O', 'big'),\n",
       " ('O', 'picture'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'understand'),\n",
       " ('O', 'how'),\n",
       " ('O', 'all'),\n",
       " ('O', 'the'),\n",
       " ('O', 'different'),\n",
       " ('O', 'parts'),\n",
       " ('O', 'of'),\n",
       " ('O', 'a'),\n",
       " ('O', 'project'),\n",
       " ('O', 'fit'),\n",
       " ('O', 'together'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'make'),\n",
       " ('O', 'better'),\n",
       " ('O', 'decisions'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'avoid'),\n",
       " ('O', 'making'),\n",
       " ('O', 'mistakes'),\n",
       " ('O', '.'),\n",
       " ('O', 'another'),\n",
       " ('O', 'consequence'),\n",
       " ('O', 'of'),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'is'),\n",
       " ('O', 'that'),\n",
       " ('O', 'it'),\n",
       " ('O', 'helps'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'be'),\n",
       " ('O', 'more'),\n",
       " ('O', 'efficient'),\n",
       " ('O', '.'),\n",
       " ('O', 'when'),\n",
       " ('O', 'we'),\n",
       " ('O', 'plan'),\n",
       " ('O', ','),\n",
       " ('O', 'we'),\n",
       " ('O', 'can'),\n",
       " ('O', 'break'),\n",
       " ('O', 'a'),\n",
       " ('O', 'big'),\n",
       " ('O', 'task'),\n",
       " ('O', 'down'),\n",
       " ('O', 'into'),\n",
       " ('O', 'smaller'),\n",
       " ('O', 'tasks'),\n",
       " ('O', 'and'),\n",
       " ('O', 'then'),\n",
       " ('O', 'work'),\n",
       " ('O', 'on'),\n",
       " ('O', 'each'),\n",
       " ('O', 'task'),\n",
       " ('O', 'one'),\n",
       " ('O', 'at'),\n",
       " ('O', 'a'),\n",
       " ('O', 'time'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'stay'),\n",
       " ('O', 'focused'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'avoid'),\n",
       " ('O', 'feeling'),\n",
       " ('O', 'overwhelmed'),\n",
       " ('O', '.'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'can'),\n",
       " ('O', 'also'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'identify'),\n",
       " ('O', 'potential'),\n",
       " ('O', 'problems'),\n",
       " ('O', 'before'),\n",
       " ('O', 'they'),\n",
       " ('O', 'occur'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'come'),\n",
       " ('O', 'up'),\n",
       " ('O', 'with'),\n",
       " ('O', 'solutions'),\n",
       " ('O', 'to'),\n",
       " ('O', 'those'),\n",
       " ('O', 'problems'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'save'),\n",
       " ('O', 'time'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'avoid'),\n",
       " ('O', 'wasting'),\n",
       " ('O', 'resources'),\n",
       " ('O', '.'),\n",
       " ('O', 'finally'),\n",
       " ('O', ','),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'be'),\n",
       " ('O', 'more'),\n",
       " ('O', 'successful'),\n",
       " ('O', '.'),\n",
       " ('O', 'when'),\n",
       " ('O', 'we'),\n",
       " ('O', 'plan'),\n",
       " ('O', ','),\n",
       " ('O', 'we'),\n",
       " ('O', 'are'),\n",
       " ('O', 'able'),\n",
       " ('O', 'to'),\n",
       " ('O', 'set'),\n",
       " ('O', 'clear'),\n",
       " ('O', 'goals'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'create'),\n",
       " ('O', 'a'),\n",
       " ('O', 'road'),\n",
       " ('O', '##ma'),\n",
       " ('O', '##p'),\n",
       " ('O', 'for'),\n",
       " ('O', 'how'),\n",
       " ('O', 'we'),\n",
       " ('O', 'are'),\n",
       " ('O', 'going'),\n",
       " ('O', 'to'),\n",
       " ('O', 'achieve'),\n",
       " ('O', 'those'),\n",
       " ('O', 'goals'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'stay'),\n",
       " ('O', 'motivated'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'keep'),\n",
       " ('O', 'moving'),\n",
       " ('O', 'forward'),\n",
       " ('O', ','),\n",
       " ('O', 'even'),\n",
       " ('O', 'when'),\n",
       " ('O', 'things'),\n",
       " ('O', 'get'),\n",
       " ('O', 'tough'),\n",
       " ('O', '.'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'can'),\n",
       " ('O', 'also'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'track'),\n",
       " ('O', 'our'),\n",
       " ('O', 'progress'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'see'),\n",
       " ('O', 'how'),\n",
       " ('O', 'far'),\n",
       " ('O', 'we'),\n",
       " ('O', 'have'),\n",
       " ('O', 'come'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'stay'),\n",
       " ('O', 'focused'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'keep'),\n",
       " ('O', 'working'),\n",
       " ('O', 'towards'),\n",
       " ('O', 'our'),\n",
       " ('O', 'goals'),\n",
       " ('O', ','),\n",
       " ('O', 'even'),\n",
       " ('O', 'when'),\n",
       " ('O', 'we'),\n",
       " ('O', 'face'),\n",
       " ('O', 'obstacles'),\n",
       " ('O', '.'),\n",
       " ('O', 'in'),\n",
       " ('O', 'conclusion'),\n",
       " ('O', ','),\n",
       " ('O', 'the'),\n",
       " ('O', 'consequences'),\n",
       " ('O', 'of'),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'are'),\n",
       " ('O', 'many'),\n",
       " ('O', '.'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'achieve'),\n",
       " ('O', 'our'),\n",
       " ('O', 'goals'),\n",
       " ('O', ','),\n",
       " ('O', 'to'),\n",
       " ('O', 'be'),\n",
       " ('O', 'more'),\n",
       " ('O', 'efficient'),\n",
       " ('O', ','),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'be'),\n",
       " ('O', 'more'),\n",
       " ('O', 'successful'),\n",
       " ('O', '.'),\n",
       " ('O', 'it'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'focus'),\n",
       " ('O', 'on'),\n",
       " ('O', 'what'),\n",
       " ('O', 'is'),\n",
       " ('O', 'important'),\n",
       " ('O', ','),\n",
       " ('O', 'to'),\n",
       " ('O', 'avoid'),\n",
       " ('O', 'wasting'),\n",
       " ('O', 'time'),\n",
       " ('O', ','),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'make'),\n",
       " ('O', 'better'),\n",
       " ('O', 'decisions'),\n",
       " ('O', '.'),\n",
       " ('O', 'it'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'stay'),\n",
       " ('O', 'motivated'),\n",
       " ('O', ','),\n",
       " ('O', 'to'),\n",
       " ('O', 'track'),\n",
       " ('O', 'our'),\n",
       " ('O', 'progress'),\n",
       " ('O', ','),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'keep'),\n",
       " ('O', 'moving'),\n",
       " ('O', 'forward'),\n",
       " ('O', '.'),\n",
       " ('O', 'in'),\n",
       " ('O', 'short'),\n",
       " ('O', ','),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'is'),\n",
       " ('O', 'essential'),\n",
       " ('O', 'for'),\n",
       " ('O', 'success'),\n",
       " ('O', 'in'),\n",
       " ('O', 'any'),\n",
       " ('O', 'endeavor'),\n",
       " ('O', '.'),\n",
       " ('O', 'email'),\n",
       " ('O', ':'),\n",
       " ('B-EMAIL', 'john'),\n",
       " ('B-EMAIL', '.'),\n",
       " ('B-EMAIL', 'smith'),\n",
       " ('B-EMAIL', '@'),\n",
       " ('B-EMAIL', 'b'),\n",
       " ('B-EMAIL', '##mail'),\n",
       " ('B-EMAIL', '.'),\n",
       " ('B-EMAIL', 'com'),\n",
       " ('O', 'phone'),\n",
       " ('O', ':'),\n",
       " ('B-EMAIL', '123'),\n",
       " ('I-PHONE_NUM', '-'),\n",
       " ('B-EMAIL', '45'),\n",
       " ('B-EMAIL', '##6'),\n",
       " ('I-PHONE_NUM', '-'),\n",
       " ('B-EMAIL', '78'),\n",
       " ('B-EMAIL', '##90'),\n",
       " ('O', '[SEP]')]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay = \"\"\"\n",
    "'The consequences of Great Planning', by John Smith\n",
    "\n",
    "The consequences of great planning are many. One of the most important consequences is that it allows us to achieve our goals. When we plan, we are able to think about what we want to achieve and how we are going to achieve it. This helps us to focus on what is important and to avoid wasting time on things that are not important. Planning also helps us to see the big picture and to understand how all the different parts of a project fit together. This can help us to make better decisions and to avoid making mistakes.\n",
    "\n",
    "Another consequence of great planning is that it helps us to be more efficient. When we plan, we can break a big task down into smaller tasks and then work on each task one at a time. This can help us to stay focused and to avoid feeling overwhelmed. Planning can also help us to identify potential problems before they occur and to come up with solutions to those problems. This can help us to save time and to avoid wasting resources.\n",
    "\n",
    "Finally, great planning can help us to be more successful. When we plan, we are able to set clear goals and to create a roadmap for how we are going to achieve those goals. This can help us to stay motivated and to keep moving forward, even when things get tough. Planning can also help us to track our progress and to see how far we have come. This can help us to stay focused and to keep working towards our goals, even when we face obstacles.\n",
    "\n",
    "In conclusion, the consequences of great planning are many. Planning can help us to achieve our goals, to be more efficient, and to be more successful. It can help us to focus on what is important, to avoid wasting time, and to make better decisions. It can help us to stay motivated, to track our progress, and to keep moving forward. In short, great planning is essential for success in any endeavor.\n",
    "\n",
    "email: john.smith@bmail.com\n",
    "phone: 123-456-7890\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "predictions = model(**tokenizer(essay, return_tensors='pt'))\n",
    "predictions = np.argmax(predictions.logits.detach().numpy(), axis=2)\n",
    "\n",
    "bert_tokens = tokenizer.tokenize(essay)\n",
    "bert_tokens = ['[CLS]'] + bert_tokens + ['[SEP]']\n",
    "\n",
    "[(id2label[pred], token) for pred, token in zip(predictions[0], bert_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2048b70fc4d4074b1551f2400baa060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.010430662892758846,\n",
       " 'eval_precision': 0.10179640718562874,\n",
       " 'eval_recall': 0.22869955156950672,\n",
       " 'eval_f1': 0.1408839779005525,\n",
       " 'eval_accuracy': 0.9983660777738643,\n",
       " 'eval_runtime': 56.1491,\n",
       " 'eval_samples_per_second': 22.832,\n",
       " 'eval_steps_per_second': 2.867}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_data_split[\"train\"],\n",
    "    eval_dataset=encoded_data_split[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " -100]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_flat['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_labels['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474 14 0\n",
      "475 14 0\n",
      "476 14 0\n",
      "477 14 7\n",
      "478 14 7\n",
      "['nat', '##hal', '##ie', 'sy', '##lla']\n",
      "['nat', '##hal', '##ie', 'sy', '##lla']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, (l1,l2) in enumerate(zip(data_flat['labels'][0], encoded_labels['labels'][0])):\n",
    "    if l1 != l2:\n",
    "        print(i, l1, l2)\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(encoded_labels['input_ids'][0][474:479]))\n",
    "print(tokenizer.convert_ids_to_tokens(data_flat['input_ids'][0][474:479]))\n",
    "\n",
    "data_flat['input_ids'][0] == encoded_labels['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_flat['labels'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
