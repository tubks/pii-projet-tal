{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains the unfinished, work-in-progress methods. To access the working, current evolution of the baseline check out baseline.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('..','data', 'raw', 'train.json')\n",
    "\n",
    "data = pd.read_json(data_path)\n",
    "\n",
    "data_dataset = Dataset.from_pandas(data)\n",
    "#data_dataset = data_dataset.remove_columns(['full_text','trailing_whitespace', 'document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7,\n",
       " 10,\n",
       " 16,\n",
       " 20,\n",
       " 56,\n",
       " 86,\n",
       " 93,\n",
       " 104,\n",
       " 112,\n",
       " 123,\n",
       " 136,\n",
       " 166,\n",
       " 204,\n",
       " 214,\n",
       " 269,\n",
       " 288,\n",
       " 308,\n",
       " 317,\n",
       " 324,\n",
       " 330,\n",
       " 333,\n",
       " 344,\n",
       " 356,\n",
       " 375,\n",
       " 379,\n",
       " 470,\n",
       " 472,\n",
       " 591,\n",
       " 607,\n",
       " 609,\n",
       " 616,\n",
       " 651,\n",
       " 659,\n",
       " 671,\n",
       " 714,\n",
       " 730,\n",
       " 736,\n",
       " 760,\n",
       " 828,\n",
       " 1105,\n",
       " 1134,\n",
       " 1175,\n",
       " 1185,\n",
       " 1210,\n",
       " 1221,\n",
       " 1239,\n",
       " 1277,\n",
       " 1290,\n",
       " 1295,\n",
       " 1309,\n",
       " 1325,\n",
       " 1353,\n",
       " 1437,\n",
       " 1444,\n",
       " 1447,\n",
       " 1472,\n",
       " 1477,\n",
       " 1546,\n",
       " 1549,\n",
       " 1578,\n",
       " 1613,\n",
       " 1753,\n",
       " 1758,\n",
       " 1763,\n",
       " 1769,\n",
       " 1790,\n",
       " 1795,\n",
       " 1798,\n",
       " 1802,\n",
       " 1810,\n",
       " 1814,\n",
       " 1817,\n",
       " 1824,\n",
       " 2054,\n",
       " 2058,\n",
       " 2061,\n",
       " 2335,\n",
       " 2651,\n",
       " 2660,\n",
       " 2672,\n",
       " 2694,\n",
       " 2700,\n",
       " 2711,\n",
       " 2722,\n",
       " 2732,\n",
       " 2745,\n",
       " 2769,\n",
       " 2780,\n",
       " 2790,\n",
       " 2792,\n",
       " 2797,\n",
       " 2802,\n",
       " 2804,\n",
       " 2813,\n",
       " 2828,\n",
       " 2842,\n",
       " 2882,\n",
       " 2901,\n",
       " 2915,\n",
       " 2926,\n",
       " 2955,\n",
       " 2995,\n",
       " 3003,\n",
       " 3202,\n",
       " 3211,\n",
       " 3214,\n",
       " 3231,\n",
       " 3241,\n",
       " 3249,\n",
       " 3275,\n",
       " 3294,\n",
       " 3318,\n",
       " 3328,\n",
       " 3350,\n",
       " 3351,\n",
       " 3380,\n",
       " 3391,\n",
       " 3402,\n",
       " 3423,\n",
       " 3427,\n",
       " 3446,\n",
       " 3538,\n",
       " 3540,\n",
       " 3553,\n",
       " 3565,\n",
       " 3579,\n",
       " 3586,\n",
       " 3592,\n",
       " 3603,\n",
       " 3652,\n",
       " 3664,\n",
       " 3678,\n",
       " 3686,\n",
       " 3699,\n",
       " 3709,\n",
       " 3732,\n",
       " 3736,\n",
       " 3743,\n",
       " 3756,\n",
       " 3759,\n",
       " 3786,\n",
       " 3803,\n",
       " 3835,\n",
       " 3882,\n",
       " 3885,\n",
       " 3892,\n",
       " 3894,\n",
       " 3900,\n",
       " 3911,\n",
       " 3915,\n",
       " 3921,\n",
       " 3935,\n",
       " 3942,\n",
       " 3968,\n",
       " 3987,\n",
       " 3995,\n",
       " 4004,\n",
       " 4040,\n",
       " 4059,\n",
       " 4061,\n",
       " 4070,\n",
       " 4090,\n",
       " 4094,\n",
       " 4115,\n",
       " 4158,\n",
       " 4161,\n",
       " 4176,\n",
       " 4185,\n",
       " 4191,\n",
       " 4214,\n",
       " 4219,\n",
       " 4227,\n",
       " 4236,\n",
       " 4263,\n",
       " 4276,\n",
       " 4278,\n",
       " 4287,\n",
       " 4295,\n",
       " 4313,\n",
       " 4317,\n",
       " 4323,\n",
       " 4328,\n",
       " 4336,\n",
       " 4351,\n",
       " 4374,\n",
       " 4381,\n",
       " 4394,\n",
       " 4403,\n",
       " 4413,\n",
       " 4418,\n",
       " 4433,\n",
       " 4438,\n",
       " 4442,\n",
       " 4452,\n",
       " 4462,\n",
       " 4465,\n",
       " 4486,\n",
       " 4501,\n",
       " 4509,\n",
       " 4521,\n",
       " 4545,\n",
       " 4553,\n",
       " 4561,\n",
       " 4574,\n",
       " 4600,\n",
       " 4621,\n",
       " 4634,\n",
       " 4650,\n",
       " 4669,\n",
       " 4684,\n",
       " 4698,\n",
       " 4700,\n",
       " 4709,\n",
       " 4717,\n",
       " 4740,\n",
       " 4744,\n",
       " 4750,\n",
       " 4765,\n",
       " 4769,\n",
       " 4777,\n",
       " 4784,\n",
       " 4799,\n",
       " 4810,\n",
       " 4816,\n",
       " 4825,\n",
       " 4834,\n",
       " 4842,\n",
       " 4855,\n",
       " 4868,\n",
       " 4878,\n",
       " 4899,\n",
       " 4913,\n",
       " 4922,\n",
       " 4936,\n",
       " 4951,\n",
       " 4956,\n",
       " 4968,\n",
       " 4971,\n",
       " 4998,\n",
       " 5001,\n",
       " 5023,\n",
       " 5037,\n",
       " 5044,\n",
       " 5054,\n",
       " 5060,\n",
       " 5067,\n",
       " 5069,\n",
       " 5083,\n",
       " 5085,\n",
       " 5108,\n",
       " 5116,\n",
       " 5144,\n",
       " 5167,\n",
       " 5192,\n",
       " 5209,\n",
       " 5215,\n",
       " 5219,\n",
       " 5225,\n",
       " 5233,\n",
       " 5236,\n",
       " 5249,\n",
       " 5258,\n",
       " 5263,\n",
       " 5271,\n",
       " 5287,\n",
       " 5296,\n",
       " 5301,\n",
       " 5319,\n",
       " 5322,\n",
       " 5344,\n",
       " 5361,\n",
       " 5380,\n",
       " 5397,\n",
       " 5425,\n",
       " 5433,\n",
       " 5452,\n",
       " 5470,\n",
       " 5472,\n",
       " 5480,\n",
       " 5484,\n",
       " 5494,\n",
       " 5529,\n",
       " 5537,\n",
       " 5549,\n",
       " 5561,\n",
       " 5568,\n",
       " 5576,\n",
       " 5590,\n",
       " 5596,\n",
       " 5606,\n",
       " 5613,\n",
       " 5621,\n",
       " 5628,\n",
       " 5634,\n",
       " 5641,\n",
       " 5653,\n",
       " 5662,\n",
       " 5671,\n",
       " 5688,\n",
       " 5697,\n",
       " 5716,\n",
       " 5717,\n",
       " 5731,\n",
       " 5736,\n",
       " 5738,\n",
       " 5748,\n",
       " 5775,\n",
       " 5780,\n",
       " 5796,\n",
       " 5811,\n",
       " 5816,\n",
       " 5830,\n",
       " 5840,\n",
       " 5855,\n",
       " 5874,\n",
       " 5896,\n",
       " 5902,\n",
       " 5906,\n",
       " 5910,\n",
       " 5922,\n",
       " 5923,\n",
       " 5935,\n",
       " 5944,\n",
       " 5952,\n",
       " 5964,\n",
       " 6002,\n",
       " 6007,\n",
       " 6023,\n",
       " 6027,\n",
       " 6049,\n",
       " 6057,\n",
       " 6064,\n",
       " 6074,\n",
       " 6085,\n",
       " 6088,\n",
       " 6095,\n",
       " 6101,\n",
       " 6117,\n",
       " 6121,\n",
       " 6124,\n",
       " 6133,\n",
       " 6139,\n",
       " 6148,\n",
       " 6155,\n",
       " 6169,\n",
       " 6171,\n",
       " 6174,\n",
       " 6187,\n",
       " 6194,\n",
       " 6220,\n",
       " 6243,\n",
       " 6257,\n",
       " 6270,\n",
       " 6282,\n",
       " 6296,\n",
       " 6319,\n",
       " 6339,\n",
       " 6348,\n",
       " 6353,\n",
       " 6364,\n",
       " 6378,\n",
       " 6393,\n",
       " 6416,\n",
       " 6426,\n",
       " 6432,\n",
       " 6435,\n",
       " 6450,\n",
       " 6457,\n",
       " 6459,\n",
       " 6465,\n",
       " 6489,\n",
       " 6497,\n",
       " 6517,\n",
       " 6531,\n",
       " 6537,\n",
       " 6557,\n",
       " 6577,\n",
       " 6591,\n",
       " 6595,\n",
       " 6600,\n",
       " 6611,\n",
       " 6622,\n",
       " 6632,\n",
       " 6647,\n",
       " 6657,\n",
       " 6665,\n",
       " 6681,\n",
       " 6686,\n",
       " 6694,\n",
       " 6718,\n",
       " 6728,\n",
       " 6750,\n",
       " 6754,\n",
       " 6769,\n",
       " 6775,\n",
       " 6784,\n",
       " 6790,\n",
       " 6808,\n",
       " 6830,\n",
       " 6849,\n",
       " 6853,\n",
       " 6859,\n",
       " 6874,\n",
       " 6888,\n",
       " 6898,\n",
       " 6900,\n",
       " 6904,\n",
       " 6921,\n",
       " 6941,\n",
       " 6946,\n",
       " 6954,\n",
       " 6973,\n",
       " 6976,\n",
       " 6999,\n",
       " 7000,\n",
       " 7003,\n",
       " 7006,\n",
       " 7022,\n",
       " 7025,\n",
       " 7034,\n",
       " 7037,\n",
       " 7065,\n",
       " 7075,\n",
       " 7084,\n",
       " 7098,\n",
       " 7115,\n",
       " 7124,\n",
       " 7137,\n",
       " 7156,\n",
       " 7176,\n",
       " 7197,\n",
       " 7203,\n",
       " 7222,\n",
       " 7236,\n",
       " 7250,\n",
       " 7256,\n",
       " 7262,\n",
       " 7265,\n",
       " 7271,\n",
       " 7293,\n",
       " 7308,\n",
       " 7319,\n",
       " 7321,\n",
       " 7332,\n",
       " 7352,\n",
       " 7382,\n",
       " 7404,\n",
       " 7408,\n",
       " 7411,\n",
       " 7427,\n",
       " 7438,\n",
       " 7458,\n",
       " 7469,\n",
       " 7490,\n",
       " 7504,\n",
       " 7516,\n",
       " 7537,\n",
       " 7548,\n",
       " 7581,\n",
       " 7591,\n",
       " 7604,\n",
       " 7632,\n",
       " 7645,\n",
       " 7663,\n",
       " 7668,\n",
       " 7676,\n",
       " 7680,\n",
       " 7697,\n",
       " 7708,\n",
       " 7713,\n",
       " 7730,\n",
       " 7733,\n",
       " 7735,\n",
       " 7745,\n",
       " 7754,\n",
       " 7763,\n",
       " 7770,\n",
       " 7779,\n",
       " 7786,\n",
       " 7795,\n",
       " 7804,\n",
       " 7812,\n",
       " 7834,\n",
       " 7839,\n",
       " 7849,\n",
       " 7858,\n",
       " 7865,\n",
       " 7867,\n",
       " 7880,\n",
       " 7886,\n",
       " 7909,\n",
       " 7929,\n",
       " 7932,\n",
       " 7944,\n",
       " 7959,\n",
       " 7974,\n",
       " 7993,\n",
       " 8009,\n",
       " 8011,\n",
       " 8021,\n",
       " 8028,\n",
       " 8033,\n",
       " 8070,\n",
       " 8076,\n",
       " 8095,\n",
       " 8108,\n",
       " 8123,\n",
       " 8127,\n",
       " 8139,\n",
       " 8165,\n",
       " 8192,\n",
       " 8229,\n",
       " 8236,\n",
       " 8251,\n",
       " 8266,\n",
       " 8271,\n",
       " 8296,\n",
       " 8302,\n",
       " 8316,\n",
       " 8329,\n",
       " 8335,\n",
       " 8340,\n",
       " 8344,\n",
       " 8349,\n",
       " 8364,\n",
       " 8371,\n",
       " 8379,\n",
       " 8393,\n",
       " 8416,\n",
       " 8435,\n",
       " 8447,\n",
       " 8467,\n",
       " 8474,\n",
       " 8480,\n",
       " 8485,\n",
       " 8491,\n",
       " 8505,\n",
       " 8518,\n",
       " 8545,\n",
       " 8550,\n",
       " 8562,\n",
       " 8575,\n",
       " 8578,\n",
       " 8584,\n",
       " 8593,\n",
       " 8603,\n",
       " 8609,\n",
       " 8612,\n",
       " 8621,\n",
       " 8631,\n",
       " 8642,\n",
       " 8652,\n",
       " 8669,\n",
       " 8672,\n",
       " 8674,\n",
       " 8688,\n",
       " 8690,\n",
       " 8699,\n",
       " 8711,\n",
       " 8714,\n",
       " 8718,\n",
       " 8733,\n",
       " 8736,\n",
       " 8738,\n",
       " 8741,\n",
       " 8744,\n",
       " 8747,\n",
       " 8750,\n",
       " 8751,\n",
       " 8752,\n",
       " 8753,\n",
       " 8755,\n",
       " 8757,\n",
       " 8758,\n",
       " 8759,\n",
       " 8761,\n",
       " 8762,\n",
       " 8766,\n",
       " 8768,\n",
       " 8772,\n",
       " 8773,\n",
       " 8776,\n",
       " 8778,\n",
       " 8783,\n",
       " 8785,\n",
       " 8787,\n",
       " 8794,\n",
       " 8795,\n",
       " 8797,\n",
       " 8798,\n",
       " 8800,\n",
       " 8803,\n",
       " 8805,\n",
       " 8806,\n",
       " 8811,\n",
       " 8812,\n",
       " 8813,\n",
       " 8814,\n",
       " 8816,\n",
       " 8819,\n",
       " 8820,\n",
       " 8822,\n",
       " 8829,\n",
       " 8831,\n",
       " 8833,\n",
       " 8836,\n",
       " 8837,\n",
       " 8839,\n",
       " 8842,\n",
       " 8846,\n",
       " 8848,\n",
       " 8849,\n",
       " 8850,\n",
       " 8853,\n",
       " 8854,\n",
       " 8856,\n",
       " 8857,\n",
       " 8863,\n",
       " 8865,\n",
       " 8866,\n",
       " 8867,\n",
       " 8868,\n",
       " 8871,\n",
       " 8874,\n",
       " 8875,\n",
       " 8878,\n",
       " 8880,\n",
       " 8882,\n",
       " 8883,\n",
       " 8885,\n",
       " 8887,\n",
       " 8888,\n",
       " 8889,\n",
       " 8891,\n",
       " 8892,\n",
       " 8894,\n",
       " 8895,\n",
       " 8898,\n",
       " 8902,\n",
       " 8905,\n",
       " 8908,\n",
       " 8909,\n",
       " 8910,\n",
       " 8918,\n",
       " 8920,\n",
       " 8921,\n",
       " 8923,\n",
       " 8925,\n",
       " 8928,\n",
       " 8929,\n",
       " 8934,\n",
       " 8935,\n",
       " 8936,\n",
       " 8937,\n",
       " 8938,\n",
       " 8942,\n",
       " 8943,\n",
       " 8944,\n",
       " 8947,\n",
       " 8948,\n",
       " 8950,\n",
       " 8952,\n",
       " 8954,\n",
       " 8956,\n",
       " 8960,\n",
       " 8962,\n",
       " 8965,\n",
       " 8966,\n",
       " 8968,\n",
       " 8970,\n",
       " 8973,\n",
       " 8975,\n",
       " 8976,\n",
       " 8978,\n",
       " 8979,\n",
       " 8980,\n",
       " 8982,\n",
       " 8984,\n",
       " 8986,\n",
       " 8987,\n",
       " 8988,\n",
       " 8989,\n",
       " 8993,\n",
       " 8995,\n",
       " 8996,\n",
       " 8997,\n",
       " 9000,\n",
       " 9002,\n",
       " 9004,\n",
       " 9006,\n",
       " 9007,\n",
       " 9009,\n",
       " 9015,\n",
       " 9016,\n",
       " 9018,\n",
       " 9019,\n",
       " 9021,\n",
       " 9025,\n",
       " 9027,\n",
       " 9028,\n",
       " 9031,\n",
       " 9032,\n",
       " 9033,\n",
       " 9034,\n",
       " 9036,\n",
       " 9038,\n",
       " 9040,\n",
       " 9042,\n",
       " 9043,\n",
       " 9045,\n",
       " 9047,\n",
       " 9049,\n",
       " 9050,\n",
       " 9055,\n",
       " 9056,\n",
       " 9058,\n",
       " 9060,\n",
       " 9061,\n",
       " 9062,\n",
       " 9063,\n",
       " 9064,\n",
       " 9066,\n",
       " 9067,\n",
       " 9070,\n",
       " 9072,\n",
       " 9074,\n",
       " 9075,\n",
       " 9077,\n",
       " 9078,\n",
       " 9079,\n",
       " 9080,\n",
       " 9081,\n",
       " 9085,\n",
       " 9086,\n",
       " 9087,\n",
       " 9088,\n",
       " 9095,\n",
       " 9097,\n",
       " 9098,\n",
       " 9099,\n",
       " 9100,\n",
       " 9101,\n",
       " 9103,\n",
       " 9106,\n",
       " 9109,\n",
       " 9110,\n",
       " 9113,\n",
       " 9115,\n",
       " 9117,\n",
       " 9127,\n",
       " 9128,\n",
       " 9132,\n",
       " 9134,\n",
       " 9137,\n",
       " 9138,\n",
       " 9140,\n",
       " 9141,\n",
       " 9145,\n",
       " 9146,\n",
       " 9148,\n",
       " 9151,\n",
       " 9152,\n",
       " 9154,\n",
       " 9157,\n",
       " 9158,\n",
       " 9160,\n",
       " 9161,\n",
       " 9165,\n",
       " 9167,\n",
       " 9169,\n",
       " 9171,\n",
       " 9172,\n",
       " 9174,\n",
       " 9175,\n",
       " 9176,\n",
       " 9180,\n",
       " 9181,\n",
       " 9182,\n",
       " 9183,\n",
       " 9185,\n",
       " 9187,\n",
       " 9188,\n",
       " 9189,\n",
       " 9190,\n",
       " 9191,\n",
       " 9192,\n",
       " 9194,\n",
       " 9195,\n",
       " 9196,\n",
       " 9197,\n",
       " 9201,\n",
       " 9203,\n",
       " 9208,\n",
       " 9212,\n",
       " 9213,\n",
       " 9214,\n",
       " 9215,\n",
       " 9216,\n",
       " 9219,\n",
       " 9221,\n",
       " 9223,\n",
       " 9224,\n",
       " 9225,\n",
       " 9226,\n",
       " 9228,\n",
       " 9236,\n",
       " 9239,\n",
       " 9241,\n",
       " 9243,\n",
       " 9245,\n",
       " 9247,\n",
       " 9251,\n",
       " 9255,\n",
       " 9257,\n",
       " 9258,\n",
       " 9260,\n",
       " 9261,\n",
       " 9262,\n",
       " 9263,\n",
       " 9268,\n",
       " 9270,\n",
       " 9272,\n",
       " 9274,\n",
       " 9275,\n",
       " 9278,\n",
       " 9281,\n",
       " 9286,\n",
       " 9289,\n",
       " 9290,\n",
       " 9294,\n",
       " 9295,\n",
       " 9297,\n",
       " 9298,\n",
       " 9301,\n",
       " 9307,\n",
       " 9310,\n",
       " 9311,\n",
       " 9313,\n",
       " 9314,\n",
       " 9316,\n",
       " 9318,\n",
       " 9319,\n",
       " 9324,\n",
       " 9325,\n",
       " 9327,\n",
       " 9328,\n",
       " 9329,\n",
       " 9335,\n",
       " 9336,\n",
       " 9338,\n",
       " 9341,\n",
       " 9344,\n",
       " 9345,\n",
       " 9346,\n",
       " 9347,\n",
       " 9348,\n",
       " 9349,\n",
       " 9350,\n",
       " 9352,\n",
       " 9353,\n",
       " 9354,\n",
       " 9358,\n",
       " 9359,\n",
       " 9360,\n",
       " 9361,\n",
       " 9362,\n",
       " 9365,\n",
       " 9367,\n",
       " 9368,\n",
       " 9374,\n",
       " 9377,\n",
       " 9385,\n",
       " 9386,\n",
       " 9387,\n",
       " 9389,\n",
       " 9390,\n",
       " 9391,\n",
       " 9394,\n",
       " 9397,\n",
       " 9399,\n",
       " 9402,\n",
       " 9404,\n",
       " 9405,\n",
       " 9410,\n",
       " 9411,\n",
       " 9414,\n",
       " 9415,\n",
       " 9416,\n",
       " 9419,\n",
       " 9420,\n",
       " 9421,\n",
       " 9422,\n",
       " 9424,\n",
       " 9425,\n",
       " 9428,\n",
       " 9433,\n",
       " 9434,\n",
       " 9440,\n",
       " 9444,\n",
       " 9447,\n",
       " 9449,\n",
       " 9453,\n",
       " 9459,\n",
       " 9460,\n",
       " 9461,\n",
       " 9464,\n",
       " 9465,\n",
       " 9469,\n",
       " 9470,\n",
       " 9471,\n",
       " 9473,\n",
       " 9478,\n",
       " 9479,\n",
       " 9480,\n",
       " 9481,\n",
       " 9485,\n",
       " 9488,\n",
       " 9489,\n",
       " 9490,\n",
       " 9491,\n",
       " 9493,\n",
       " 9494,\n",
       " 9495,\n",
       " 9497,\n",
       " 9498,\n",
       " 9500,\n",
       " 9501,\n",
       " 9502,\n",
       " 9503,\n",
       " 9504,\n",
       " 9505,\n",
       " 9506,\n",
       " 9508,\n",
       " 9509,\n",
       " 9510,\n",
       " 9512,\n",
       " 9513,\n",
       " 9517,\n",
       " 9518,\n",
       " 9519,\n",
       " 9520,\n",
       " 9527,\n",
       " 9530,\n",
       " 9531,\n",
       " 9532,\n",
       " 9533,\n",
       " 9535,\n",
       " 9536,\n",
       " 9539,\n",
       " 9540,\n",
       " 9541,\n",
       " 9543,\n",
       " 9545,\n",
       " 9547,\n",
       " 9548,\n",
       " 9549,\n",
       " 9550,\n",
       " 9551,\n",
       " 9553,\n",
       " 9558,\n",
       " 9560,\n",
       " 9563,\n",
       " 9565,\n",
       " 9567,\n",
       " 9569,\n",
       " 9571,\n",
       " 9575,\n",
       " 9578,\n",
       " 9580,\n",
       " 9581,\n",
       " 9582,\n",
       " 9583,\n",
       " 9584,\n",
       " 9585,\n",
       " 9587,\n",
       " 9588,\n",
       " 9589,\n",
       " 9590,\n",
       " 9591,\n",
       " 9593,\n",
       " 9594,\n",
       " 9595,\n",
       " 9600,\n",
       " 9609,\n",
       " 9610,\n",
       " 9611,\n",
       " 9612,\n",
       " 9615,\n",
       " 9616,\n",
       " 9621,\n",
       " 9623,\n",
       " 9625,\n",
       " 9629,\n",
       " 9630,\n",
       " 9636,\n",
       " 9637,\n",
       " 9639,\n",
       " 9640,\n",
       " 9641,\n",
       " 9644,\n",
       " ...]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dataset['document']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer(data_dataset['full_text'], padding=True, truncation=True, return_overflowing_tokens=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]ctions. this second workshop also lasts two hours and allows the mind map to evolve. once familiarized with it, the stakeholders discover the power of the tool. then, the second workshop brings out even more ideas and constructive exchanges between the stakeholders. around this new mind map, they have learned to work together and want to make visible the untold ideas. i now present all the projects i manage in this type of format in order to ease rapid understanding for decision - makers. these presentations are the core of my business models. the decision - makers are thus able to identify the opportunities of the projects and can take quick decisions to validate them. they find answers to their questions thank to a schematic representation. approach what i find amazing with the facilitation of this type of workshop is the participants commitment for the project. this tool helps to give meaning. the participants appropriate the story and want to keep writing it. then, they easily become actors or sponsors of the project. a trust relationship is built, thus facilitating the implementation of related actions. design thinking for innovation reflexion - avril 2021 - nathalie sylla annex 1 : mind map shared facilities project [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized['input_ids'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 9,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 58,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 80,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 140,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 200,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217,\n",
       " 218,\n",
       " 219,\n",
       " 220,\n",
       " 221,\n",
       " 222,\n",
       " 223,\n",
       " 224,\n",
       " 225,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 229,\n",
       " 230,\n",
       " 231,\n",
       " 232,\n",
       " 233,\n",
       " 234,\n",
       " 235,\n",
       " 236,\n",
       " 237,\n",
       " 238,\n",
       " 239,\n",
       " 240,\n",
       " 240,\n",
       " 240,\n",
       " 241,\n",
       " 242,\n",
       " 243,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 247,\n",
       " 248,\n",
       " 249,\n",
       " 250,\n",
       " 251,\n",
       " 252,\n",
       " 253,\n",
       " 254,\n",
       " 255,\n",
       " 256,\n",
       " 257,\n",
       " 258,\n",
       " 259,\n",
       " 260,\n",
       " 261,\n",
       " 262,\n",
       " 263,\n",
       " 264,\n",
       " 265,\n",
       " 266,\n",
       " 267,\n",
       " 268,\n",
       " 269,\n",
       " 270,\n",
       " 271,\n",
       " 272,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 276,\n",
       " 277,\n",
       " 278,\n",
       " 279,\n",
       " 280,\n",
       " 281,\n",
       " 282,\n",
       " 283,\n",
       " 284,\n",
       " 285,\n",
       " 286,\n",
       " 287,\n",
       " 288,\n",
       " 289,\n",
       " 290,\n",
       " 291,\n",
       " 292,\n",
       " 293,\n",
       " 294,\n",
       " 295,\n",
       " 296,\n",
       " 297,\n",
       " 298,\n",
       " 299,\n",
       " 300,\n",
       " 301,\n",
       " 302,\n",
       " 303,\n",
       " 304,\n",
       " 305,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 309,\n",
       " 310,\n",
       " 311,\n",
       " 312,\n",
       " 313,\n",
       " 314,\n",
       " 315,\n",
       " 316,\n",
       " 317,\n",
       " 318,\n",
       " 319,\n",
       " 320,\n",
       " 321,\n",
       " 322,\n",
       " 323,\n",
       " 324,\n",
       " 325,\n",
       " 326,\n",
       " 327,\n",
       " 328,\n",
       " 329,\n",
       " 330,\n",
       " 331,\n",
       " 332,\n",
       " 333,\n",
       " 334,\n",
       " 335,\n",
       " 336,\n",
       " 337,\n",
       " 338,\n",
       " 339,\n",
       " 340,\n",
       " 341,\n",
       " 342,\n",
       " 343,\n",
       " 344,\n",
       " 345,\n",
       " 346,\n",
       " 347,\n",
       " 348,\n",
       " 349,\n",
       " 350,\n",
       " 351,\n",
       " 352,\n",
       " 353,\n",
       " 354,\n",
       " 355,\n",
       " 356,\n",
       " 357,\n",
       " 358,\n",
       " 359,\n",
       " 360,\n",
       " 361,\n",
       " 362,\n",
       " 363,\n",
       " 364,\n",
       " 365,\n",
       " 366,\n",
       " 367,\n",
       " 368,\n",
       " 369,\n",
       " 370,\n",
       " 371,\n",
       " 372,\n",
       " 373,\n",
       " 374,\n",
       " 375,\n",
       " 376,\n",
       " 377,\n",
       " 378,\n",
       " 379,\n",
       " 380,\n",
       " 381,\n",
       " 382,\n",
       " 383,\n",
       " 384,\n",
       " 385,\n",
       " 386,\n",
       " 387,\n",
       " 388,\n",
       " 389,\n",
       " 390,\n",
       " 391,\n",
       " 392,\n",
       " 393,\n",
       " 394,\n",
       " 395,\n",
       " 396,\n",
       " 397,\n",
       " 398,\n",
       " 399,\n",
       " 400,\n",
       " 401,\n",
       " 402,\n",
       " 403,\n",
       " 404,\n",
       " 405,\n",
       " 406,\n",
       " 407,\n",
       " 408,\n",
       " 409,\n",
       " 410,\n",
       " 411,\n",
       " 411,\n",
       " 412,\n",
       " 413,\n",
       " 414,\n",
       " 414,\n",
       " 415,\n",
       " 416,\n",
       " 417,\n",
       " 417,\n",
       " 418,\n",
       " 419,\n",
       " 420,\n",
       " 421,\n",
       " 422,\n",
       " 423,\n",
       " 424,\n",
       " 425,\n",
       " 426,\n",
       " 427,\n",
       " 428,\n",
       " 429,\n",
       " 430,\n",
       " 431,\n",
       " 432,\n",
       " 433,\n",
       " 434,\n",
       " 435,\n",
       " 436,\n",
       " 437,\n",
       " 438,\n",
       " 439,\n",
       " 439,\n",
       " 440,\n",
       " 441,\n",
       " 441,\n",
       " 442,\n",
       " 443,\n",
       " 444,\n",
       " 444,\n",
       " 444,\n",
       " 445,\n",
       " 445,\n",
       " 446,\n",
       " 447,\n",
       " 448,\n",
       " 449,\n",
       " 450,\n",
       " 451,\n",
       " 452,\n",
       " 453,\n",
       " 454,\n",
       " 455,\n",
       " 456,\n",
       " 457,\n",
       " 458,\n",
       " 459,\n",
       " 460,\n",
       " 461,\n",
       " 461,\n",
       " 462,\n",
       " 463,\n",
       " 464,\n",
       " 465,\n",
       " 466,\n",
       " 467,\n",
       " 468,\n",
       " 469,\n",
       " 470,\n",
       " 471,\n",
       " 472,\n",
       " 473,\n",
       " 474,\n",
       " 474,\n",
       " 474,\n",
       " None]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized.word_ids(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-NAME_STUDENT',\n",
       " 1: 'B-EMAIL',\n",
       " 2: 'B-USERNAME',\n",
       " 3: 'B-ID_NUM',\n",
       " 4: 'B-PHONE_NUM',\n",
       " 5: 'B-URL_PERSONAL',\n",
       " 6: 'B-STREET_ADDRESS',\n",
       " 7: 'I-NAME_STUDENT',\n",
       " 8: 'I-EMAIL',\n",
       " 9: 'I-USERNAME',\n",
       " 10: 'I-ID_NUM',\n",
       " 11: 'I-PHONE_NUM',\n",
       " 12: 'I-URL_PERSONAL',\n",
       " 13: 'I-STREET_ADDRESS',\n",
       " 14: 'O',\n",
       " -100: '[PAD]'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS_LIST = ['B-NAME_STUDENT', 'B-EMAIL', 'B-USERNAME', 'B-ID_NUM', 'B-PHONE_NUM', 'B-URL_PERSONAL', 'B-STREET_ADDRESS', 'I-NAME_STUDENT', 'I-EMAIL', 'I-USERNAME', 'I-ID_NUM', 'I-PHONE_NUM','I-URL_PERSONAL','I-STREET_ADDRESS', 'O']\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(LABELS_LIST)}\n",
    "label2id['[PAD]'] = -100\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - version 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(example):\n",
    "    \"\"\"\n",
    "    to be used with datasets.map() with batched=False\n",
    "    \n",
    "    Encodes the labels into integers.\n",
    "    \n",
    "    \"\"\"\n",
    "    labels = example['labels']\n",
    "    encoded = [label2id[label] for label in labels]\n",
    "    return {'labels': encoded}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "339b3a09f8134930a384bdb2d13bda9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_labels_encoded = data_dataset.map(encode_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels'],\n",
       "    num_rows: 6807\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef0a381cd814c23b9286fd249581610",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tokenize_and_align(example, overlap_size = 0):\n",
    "    \"\"\"\n",
    "    To be used with datasets.map() with batched=False\n",
    "\n",
    "    Takes in \n",
    "        - example : an example from the datasets class\n",
    "        - overlap_size: the number of tokens that overlap between two consecutive chunks\n",
    "        \n",
    "    outputs:\n",
    "        - a Dict[]->List with columns:\n",
    "            - of the bert tokenizer output\n",
    "            - encoded labels\n",
    "    \"\"\"\n",
    "\n",
    "    org_labels = example['labels']\n",
    "    tokenized_inputs = tokenizer(example['tokens'], is_split_into_words=True, return_offsets_mapping=True, truncation=True, padding='max_length', max_length=512, return_overflowing_tokens=True, stride=overlap_size, return_tensors='pt')\n",
    "    tokenized_inputs.pop('overflow_to_sample_mapping')\n",
    "    tokenized_inputs.pop('offset_mapping')\n",
    "    \n",
    "    new_labels = []\n",
    "    org_word_ids_list = []\n",
    "    document_id = []\n",
    "    #iterating over chunks\n",
    "    for i, chunk in enumerate(tokenized_inputs['input_ids']):\n",
    "        ids_of_tokens = tokenized_inputs.word_ids(i)\n",
    "        \n",
    "        org_word_ids_list.append(ids_of_tokens)\n",
    "        document_id.append(example['document'])\n",
    "        #iterating over ids of tokens\n",
    "        chunk_labels = []\n",
    "        for id in ids_of_tokens:\n",
    "            #if id=None, then it means it's some BERT token (CLS, SEP or PAD)\n",
    "            if id is None:\n",
    "                chunk_labels.append(-100)\n",
    "            else:\n",
    "                chunk_labels.append(org_labels[id])\n",
    "        new_labels.append(chunk_labels)\n",
    "\n",
    "    tokenized_inputs['labels'] = new_labels\n",
    "    tokenized_inputs['org_word_ids'] = org_word_ids_list\n",
    "    tokenized_inputs['document'] = document_id\n",
    "\n",
    "    return tokenized_inputs\n",
    "    \n",
    "data_small = data_labels_encoded.select(range(1))\n",
    "data_small = data_small.map(tokenize_and_align, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "932e90efe8c94c2e8efeaa2bdab60443",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_encoded_all = data_labels_encoded.map(tokenize_and_align, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'full_text', 'tokens', 'trailing_whitespace', 'labels', 'input_ids', 'token_type_ids', 'attention_mask', 'offset_mapping', 'overflow_to_sample_mapping', 'new_labels'],\n",
       "    num_rows: 6807\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_encoded_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 7]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_encoded_all['document'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'org_word_ids', 'document'],\n",
       "    num_rows: 12812\n",
       "})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flatten_data(data, keys_to_flatten):\n",
    "\n",
    "    data_flat = {}\n",
    "\n",
    "    for key in keys_to_flatten:\n",
    "        data_flat[key] = reduce(lambda x,y: x+y, data[key])\n",
    "\n",
    "\n",
    "    return Dataset.from_dict(data_flat)\n",
    "\n",
    "keys_to_flatten = ['labels', 'input_ids', 'token_type_ids', 'attention_mask', 'org_word_ids','document']\n",
    "\n",
    "\n",
    "data_flat = flatten_data(data_encoded_all, keys_to_flatten)\n",
    "\n",
    "data_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([25682, 1011, 14085, 8865, 2666], [14, 14, 0, 0, 0], '2021 - nathalie')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_small['input_ids'][0][0][10:15], data_small['new_labels'][0][0][10:15], tokenizer.decode(data_small['input_ids'][0][0][10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_small['input_ids'][0][1]), len(data_small['new_labels'][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_labels = reduce(lambda x,y: x+y, data_small['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_chunked_encoded = data_labels_encoded.map(lambda x: tokenize_and_align(x,sub_word_labeling=True), batched=False)\n",
    "#time to tokenize and chunk WITH subword labeling : 1m58s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels aligned with tokens, with subword labeling\n",
      "['thinking', 'for', 'innovation', 'reflex', '##ion', '-', 'av', '##ril', '2021', '-', 'nat', '##hal']\n",
      "[14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"labels aligned with tokens, with subword labeling\")\n",
    "print(tokenizer.convert_ids_to_tokens(data_chunked_encoded['input_ids'][0][0][2:14]))\n",
    "print(data_chunked_encoded['labels'][0][0][2:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d5b7aaf1a142fb9236344d0f8a4294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_chunked_encoded_sub_labeling_false = data_labels_encoded.map(lambda x: tokenize_and_align(x,sub_word_labeling=False), batched=False)\n",
    "#time to tokenize and chunk WITHOUT subword labeling : 46s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels aligned with tokens, without subword labeling\n",
      "['thinking', 'for', 'innovation', 'reflex', '##ion', '-', 'av', '##ril', '2021', '-', 'nat', '##hal']\n",
      "[14, 14, 14, 14, -100, 14, 14, -100, 14, 14, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "print(\"labels aligned with tokens, without subword labeling\")\n",
    "print(tokenizer.convert_ids_to_tokens(data_chunked_encoded_sub_labeling_false['input_ids'][0][0][2:14]))\n",
    "print(data_chunked_encoded_sub_labeling_false['labels'][0][0][2:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#flatten the dataset\n",
    "data_flat = {}\n",
    "\n",
    "keys = ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "for key in keys:\n",
    "    data_flat[key] = reduce(lambda x,y: x+y, data_chunked_encoded[key])\n",
    "\n",
    "data_flat = Dataset.from_dict(data_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if each row has length 512 for each column\n",
      "All good\n"
     ]
    }
   ],
   "source": [
    "#checking if each row has length 512 for each column\n",
    "print(\"Checking if each row has length 512 for each column\")\n",
    "for example in data_flat:\n",
    "    for key in keys:\n",
    "        if len(example[key]) != 512:\n",
    "            print(f\"Error in {key}, length is {len(example[key])} instead of 512\")\n",
    "            break\n",
    "print(\"All good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_flat['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing - version 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(examples):\n",
    "    labels = []\n",
    "    tokenized_sentence = []\n",
    "    for word, label in zip(examples['tokens'], examples['labels']):\n",
    "        #tokenizes the word using BERT's subword tokenizer\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        #adds the same label to all the subwords of the word\n",
    "        labels.extend([label] * n_subwords)\n",
    "    examples['tokens'] = tokenized_sentence\n",
    "    examples['labels'] = labels\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7388df55e2dc4368a24b1768fd957c29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = data_dataset.map(tokenize_and_preserve_labels,batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(examples, block_size=510, sliding_window=False):\n",
    "    tokenized_sentences = []\n",
    "    labels = []\n",
    "    for i in range(0, len(examples['tokens']), block_size):\n",
    "        chunk_token = examples['tokens'][i:i+block_size]\n",
    "        chunk_label = examples['labels'][i:i+block_size]\n",
    "        if len(chunk_token) < block_size:\n",
    "            chunk_token += ['[PAD]'] * (block_size - len(chunk_token))\n",
    "            chunk_label += ['[PAD]'] * (block_size - len(chunk_label))\n",
    "        tokenized_sentences.append(chunk_token)\n",
    "        labels.append(chunk_label)\n",
    "    return {'tokens': tokenized_sentences, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d80b42c1c1435582b3f12c159b5f95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunked_data = tokenized_data.map(chunk, batched=False)\n",
    "chunked_data = chunked_data.remove_columns(['document', 'full_text','trailing_whitespace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "flat_chunks_tokens = list(reduce(lambda x, y: x + y, chunked_data['tokens'], []))\n",
    "flat_chunks_labels = list(reduce(lambda x, y: x + y, chunked_data['labels'], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_flattened_data = Dataset.from_dict({'tokens': flat_chunks_tokens, 'labels': flat_chunks_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the '[PAD]' label to be encoded as -100 so that it's ignored by the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'labels'],\n",
       "    num_rows: 12812\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_flattened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(example):\n",
    "    detokenized = list(map(lambda x: ' '.join(x), example['tokens']))\n",
    "    detokenized = list(map(lambda x: x.replace(' ##', ''), detokenized))\n",
    "    encoded = tokenizer(detokenized, truncation=True, is_split_into_words = False, return_tensors='pt')\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a80723d742884fa99d52d4c2756d7a42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_data = chunked_flattened_data.map(encode_tokens, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02d6681c5ffb42dfb5406c2944fd6234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def encode_labels(example):\n",
    "    labels = example['labels']\n",
    "    #adding -100 for the [CLS] token and [SEP] token\n",
    "    encoded = [-100] + [label2id[label] for label in labels] + [-100]\n",
    "    return {'labels': encoded}\n",
    "\n",
    "encoded_labels = encoded_data.map(encode_labels, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 12812\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(510, 512, 512)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_labels['tokens'][0]), len(encoded_labels['labels'][0]), len(encoded_labels['input_ids'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_split = encoded_labels.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- nathalie sylla challenge & selection'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_labels['input_ids'][0][11:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 11530\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1282\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_data_split = data_flat.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [LABELS_LIST[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [LABELS_LIST[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=len(id2label), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freezing the BERT layers\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b14a647a5584fc498c7c895cd02fec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0005, 'learning_rate': 4.4220989366620436e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0005, 'learning_rate': 3.844197873324087e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696205f23f7c42a09c560424aa5f5ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.00029877974884584546, 'eval_precision': 0.9052132701421801, 'eval_recall': 0.9646464646464646, 'eval_f1': 0.9339853300733496, 'eval_accuracy': 0.9998955729182983, 'eval_runtime': 53.9588, 'eval_samples_per_second': 23.759, 'eval_steps_per_second': 2.984, 'epoch': 1.0}\n",
      "{'loss': 0.0006, 'learning_rate': 3.266296809986131e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0006, 'learning_rate': 2.688395746648174e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0004, 'learning_rate': 2.1104946833102173e-05, 'epoch': 1.73}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52de4190c4b6415eb7bcf1dc685993cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0002911491028498858, 'eval_precision': 0.9073634204275535, 'eval_recall': 0.9646464646464646, 'eval_f1': 0.9351285189718482, 'eval_accuracy': 0.9998976614599324, 'eval_runtime': 53.6274, 'eval_samples_per_second': 23.906, 'eval_steps_per_second': 3.002, 'epoch': 2.0}\n",
      "{'loss': 0.0005, 'learning_rate': 1.5325936199722607e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0005, 'learning_rate': 9.546925566343042e-06, 'epoch': 2.43}\n",
      "{'loss': 0.0005, 'learning_rate': 3.7679149329634766e-06, 'epoch': 2.77}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078ed1e2c91447d89a71a0cbcdb824aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0002923872380051762, 'eval_precision': 0.9073634204275535, 'eval_recall': 0.9646464646464646, 'eval_f1': 0.9351285189718482, 'eval_accuracy': 0.9998976614599324, 'eval_runtime': 53.2128, 'eval_samples_per_second': 24.092, 'eval_steps_per_second': 3.026, 'epoch': 3.0}\n",
      "{'train_runtime': 1946.9332, 'train_samples_per_second': 17.766, 'train_steps_per_second': 2.222, 'train_loss': 0.0004889280520210759, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4326, training_loss=0.0004889280520210759, metrics={'train_runtime': 1946.9332, 'train_samples_per_second': 17.766, 'train_steps_per_second': 2.222, 'train_loss': 0.0004889280520210759, 'epoch': 3.0})"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"model_baseline_v0.1\",\n",
    "#     learning_rate=2e-3,\n",
    "#     per_device_train_batch_size=20,\n",
    "#     per_device_eval_batch_size=20,\n",
    "#     num_train_epochs=2,\n",
    "#     weight_decay=0.01,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "# )\n",
    "\n",
    "\n",
    "target_dir = \"model/trainer_model_initial_preprocessing\"\n",
    "\n",
    "training_args = TrainingArguments(output_dir=target_dir, evaluation_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_data_split[\"train\"],\n",
    "    eval_dataset=encoded_data_split[\"test\"],\n",
    "    #tokenizer=tokenizer,\n",
    "    #data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('model/model_initial_preprocessing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=16, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#put trainer on cpu\n",
    "\n",
    "trainer.model.to('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0017704bc38c41f9b501d3e844741b25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.0002923872380051762,\n",
       " 'eval_precision': 0.9073634204275535,\n",
       " 'eval_recall': 0.9646464646464646,\n",
       " 'eval_f1': 0.9351285189718482,\n",
       " 'eval_accuracy': 0.9998976614599324,\n",
       " 'eval_runtime': 54.1181,\n",
       " 'eval_samples_per_second': 23.689,\n",
       " 'eval_steps_per_second': 2.975,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = 'my name is John Smith and my email is john.smith@gmail.com.'\n",
    "\n",
    "model = model.to('cpu')\n",
    "\n",
    "predictions = model(**tokenizer(sen, return_tensors='pt'))\n",
    "predictions = np.argmax(predictions.logits.detach().numpy(), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokens = tokenizer.tokenize(sen)\n",
    "bert_tokens = ['[CLS]'] + bert_tokens + ['[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14, 14, 14, 14,  0,  7, 14, 14, 14, 14,  0,  1,  1,  1,  1,  1,\n",
       "         1,  1, 14, 14]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', '[CLS]'),\n",
       " ('O', 'my'),\n",
       " ('O', 'name'),\n",
       " ('O', 'is'),\n",
       " ('B-NAME_STUDENT', 'john'),\n",
       " ('I-NAME_STUDENT', 'smith'),\n",
       " ('O', 'and'),\n",
       " ('O', 'my'),\n",
       " ('O', 'email'),\n",
       " ('O', 'is'),\n",
       " ('B-NAME_STUDENT', 'john'),\n",
       " ('B-EMAIL', '.'),\n",
       " ('B-EMAIL', 'smith'),\n",
       " ('B-EMAIL', '@'),\n",
       " ('B-EMAIL', 'gma'),\n",
       " ('B-EMAIL', '##il'),\n",
       " ('B-EMAIL', '.'),\n",
       " ('B-EMAIL', 'com'),\n",
       " ('O', '.'),\n",
       " ('O', '[SEP]')]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(id2label[pred], token) for pred, token in zip(predictions[0], bert_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model into a pipeline and evaluaye it on the test set\n",
    "\n",
    "from transformers import TokenClassificationPipeline\n",
    "\n",
    "model_loaded = AutoModelForTokenClassification.from_pretrained('model/model_initial_preprocessing')\n",
    "model_loaded = model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', '[CLS]'),\n",
       " ('O', \"'\"),\n",
       " ('O', 'the'),\n",
       " ('O', 'consequences'),\n",
       " ('O', 'of'),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', \"'\"),\n",
       " ('O', ','),\n",
       " ('O', 'by'),\n",
       " ('B-NAME_STUDENT', 'john'),\n",
       " ('I-NAME_STUDENT', 'smith'),\n",
       " ('O', 'the'),\n",
       " ('O', 'consequences'),\n",
       " ('O', 'of'),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'are'),\n",
       " ('O', 'many'),\n",
       " ('O', '.'),\n",
       " ('O', 'one'),\n",
       " ('O', 'of'),\n",
       " ('O', 'the'),\n",
       " ('O', 'most'),\n",
       " ('O', 'important'),\n",
       " ('O', 'consequences'),\n",
       " ('O', 'is'),\n",
       " ('O', 'that'),\n",
       " ('O', 'it'),\n",
       " ('O', 'allows'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'achieve'),\n",
       " ('O', 'our'),\n",
       " ('O', 'goals'),\n",
       " ('O', '.'),\n",
       " ('O', 'when'),\n",
       " ('O', 'we'),\n",
       " ('O', 'plan'),\n",
       " ('O', ','),\n",
       " ('O', 'we'),\n",
       " ('O', 'are'),\n",
       " ('O', 'able'),\n",
       " ('O', 'to'),\n",
       " ('O', 'think'),\n",
       " ('O', 'about'),\n",
       " ('O', 'what'),\n",
       " ('O', 'we'),\n",
       " ('O', 'want'),\n",
       " ('O', 'to'),\n",
       " ('O', 'achieve'),\n",
       " ('O', 'and'),\n",
       " ('O', 'how'),\n",
       " ('O', 'we'),\n",
       " ('O', 'are'),\n",
       " ('O', 'going'),\n",
       " ('O', 'to'),\n",
       " ('O', 'achieve'),\n",
       " ('O', 'it'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'helps'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'focus'),\n",
       " ('O', 'on'),\n",
       " ('O', 'what'),\n",
       " ('O', 'is'),\n",
       " ('O', 'important'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'avoid'),\n",
       " ('O', 'wasting'),\n",
       " ('O', 'time'),\n",
       " ('O', 'on'),\n",
       " ('O', 'things'),\n",
       " ('O', 'that'),\n",
       " ('O', 'are'),\n",
       " ('O', 'not'),\n",
       " ('O', 'important'),\n",
       " ('O', '.'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'also'),\n",
       " ('O', 'helps'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'see'),\n",
       " ('O', 'the'),\n",
       " ('O', 'big'),\n",
       " ('O', 'picture'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'understand'),\n",
       " ('O', 'how'),\n",
       " ('O', 'all'),\n",
       " ('O', 'the'),\n",
       " ('O', 'different'),\n",
       " ('O', 'parts'),\n",
       " ('O', 'of'),\n",
       " ('O', 'a'),\n",
       " ('O', 'project'),\n",
       " ('O', 'fit'),\n",
       " ('O', 'together'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'make'),\n",
       " ('O', 'better'),\n",
       " ('O', 'decisions'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'avoid'),\n",
       " ('O', 'making'),\n",
       " ('O', 'mistakes'),\n",
       " ('O', '.'),\n",
       " ('O', 'another'),\n",
       " ('O', 'consequence'),\n",
       " ('O', 'of'),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'is'),\n",
       " ('O', 'that'),\n",
       " ('O', 'it'),\n",
       " ('O', 'helps'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'be'),\n",
       " ('O', 'more'),\n",
       " ('O', 'efficient'),\n",
       " ('O', '.'),\n",
       " ('O', 'when'),\n",
       " ('O', 'we'),\n",
       " ('O', 'plan'),\n",
       " ('O', ','),\n",
       " ('O', 'we'),\n",
       " ('O', 'can'),\n",
       " ('O', 'break'),\n",
       " ('O', 'a'),\n",
       " ('O', 'big'),\n",
       " ('O', 'task'),\n",
       " ('O', 'down'),\n",
       " ('O', 'into'),\n",
       " ('O', 'smaller'),\n",
       " ('O', 'tasks'),\n",
       " ('O', 'and'),\n",
       " ('O', 'then'),\n",
       " ('O', 'work'),\n",
       " ('O', 'on'),\n",
       " ('O', 'each'),\n",
       " ('O', 'task'),\n",
       " ('O', 'one'),\n",
       " ('O', 'at'),\n",
       " ('O', 'a'),\n",
       " ('O', 'time'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'stay'),\n",
       " ('O', 'focused'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'avoid'),\n",
       " ('O', 'feeling'),\n",
       " ('O', 'overwhelmed'),\n",
       " ('O', '.'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'can'),\n",
       " ('O', 'also'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'identify'),\n",
       " ('O', 'potential'),\n",
       " ('O', 'problems'),\n",
       " ('O', 'before'),\n",
       " ('O', 'they'),\n",
       " ('O', 'occur'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'come'),\n",
       " ('O', 'up'),\n",
       " ('O', 'with'),\n",
       " ('O', 'solutions'),\n",
       " ('O', 'to'),\n",
       " ('O', 'those'),\n",
       " ('O', 'problems'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'save'),\n",
       " ('O', 'time'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'avoid'),\n",
       " ('O', 'wasting'),\n",
       " ('O', 'resources'),\n",
       " ('O', '.'),\n",
       " ('O', 'finally'),\n",
       " ('O', ','),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'be'),\n",
       " ('O', 'more'),\n",
       " ('O', 'successful'),\n",
       " ('O', '.'),\n",
       " ('O', 'when'),\n",
       " ('O', 'we'),\n",
       " ('O', 'plan'),\n",
       " ('O', ','),\n",
       " ('O', 'we'),\n",
       " ('O', 'are'),\n",
       " ('O', 'able'),\n",
       " ('O', 'to'),\n",
       " ('O', 'set'),\n",
       " ('O', 'clear'),\n",
       " ('O', 'goals'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'create'),\n",
       " ('O', 'a'),\n",
       " ('O', 'road'),\n",
       " ('O', '##ma'),\n",
       " ('O', '##p'),\n",
       " ('O', 'for'),\n",
       " ('O', 'how'),\n",
       " ('O', 'we'),\n",
       " ('O', 'are'),\n",
       " ('O', 'going'),\n",
       " ('O', 'to'),\n",
       " ('O', 'achieve'),\n",
       " ('O', 'those'),\n",
       " ('O', 'goals'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'stay'),\n",
       " ('O', 'motivated'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'keep'),\n",
       " ('O', 'moving'),\n",
       " ('O', 'forward'),\n",
       " ('O', ','),\n",
       " ('O', 'even'),\n",
       " ('O', 'when'),\n",
       " ('O', 'things'),\n",
       " ('O', 'get'),\n",
       " ('O', 'tough'),\n",
       " ('O', '.'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'can'),\n",
       " ('O', 'also'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'track'),\n",
       " ('O', 'our'),\n",
       " ('O', 'progress'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'see'),\n",
       " ('O', 'how'),\n",
       " ('O', 'far'),\n",
       " ('O', 'we'),\n",
       " ('O', 'have'),\n",
       " ('O', 'come'),\n",
       " ('O', '.'),\n",
       " ('O', 'this'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'stay'),\n",
       " ('O', 'focused'),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'keep'),\n",
       " ('O', 'working'),\n",
       " ('O', 'towards'),\n",
       " ('O', 'our'),\n",
       " ('O', 'goals'),\n",
       " ('O', ','),\n",
       " ('O', 'even'),\n",
       " ('O', 'when'),\n",
       " ('O', 'we'),\n",
       " ('O', 'face'),\n",
       " ('O', 'obstacles'),\n",
       " ('O', '.'),\n",
       " ('O', 'in'),\n",
       " ('O', 'conclusion'),\n",
       " ('O', ','),\n",
       " ('O', 'the'),\n",
       " ('O', 'consequences'),\n",
       " ('O', 'of'),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'are'),\n",
       " ('O', 'many'),\n",
       " ('O', '.'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'achieve'),\n",
       " ('O', 'our'),\n",
       " ('O', 'goals'),\n",
       " ('O', ','),\n",
       " ('O', 'to'),\n",
       " ('O', 'be'),\n",
       " ('O', 'more'),\n",
       " ('O', 'efficient'),\n",
       " ('O', ','),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'be'),\n",
       " ('O', 'more'),\n",
       " ('O', 'successful'),\n",
       " ('O', '.'),\n",
       " ('O', 'it'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'focus'),\n",
       " ('O', 'on'),\n",
       " ('O', 'what'),\n",
       " ('O', 'is'),\n",
       " ('O', 'important'),\n",
       " ('O', ','),\n",
       " ('O', 'to'),\n",
       " ('O', 'avoid'),\n",
       " ('O', 'wasting'),\n",
       " ('O', 'time'),\n",
       " ('O', ','),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'make'),\n",
       " ('O', 'better'),\n",
       " ('O', 'decisions'),\n",
       " ('O', '.'),\n",
       " ('O', 'it'),\n",
       " ('O', 'can'),\n",
       " ('O', 'help'),\n",
       " ('O', 'us'),\n",
       " ('O', 'to'),\n",
       " ('O', 'stay'),\n",
       " ('O', 'motivated'),\n",
       " ('O', ','),\n",
       " ('O', 'to'),\n",
       " ('O', 'track'),\n",
       " ('O', 'our'),\n",
       " ('O', 'progress'),\n",
       " ('O', ','),\n",
       " ('O', 'and'),\n",
       " ('O', 'to'),\n",
       " ('O', 'keep'),\n",
       " ('O', 'moving'),\n",
       " ('O', 'forward'),\n",
       " ('O', '.'),\n",
       " ('O', 'in'),\n",
       " ('O', 'short'),\n",
       " ('O', ','),\n",
       " ('O', 'great'),\n",
       " ('O', 'planning'),\n",
       " ('O', 'is'),\n",
       " ('O', 'essential'),\n",
       " ('O', 'for'),\n",
       " ('O', 'success'),\n",
       " ('O', 'in'),\n",
       " ('O', 'any'),\n",
       " ('O', 'endeavor'),\n",
       " ('O', '.'),\n",
       " ('O', 'email'),\n",
       " ('O', ':'),\n",
       " ('B-EMAIL', 'john'),\n",
       " ('B-EMAIL', '.'),\n",
       " ('B-EMAIL', 'smith'),\n",
       " ('B-EMAIL', '@'),\n",
       " ('B-EMAIL', 'b'),\n",
       " ('B-EMAIL', '##mail'),\n",
       " ('B-EMAIL', '.'),\n",
       " ('B-EMAIL', 'com'),\n",
       " ('O', 'phone'),\n",
       " ('O', ':'),\n",
       " ('B-EMAIL', '123'),\n",
       " ('I-PHONE_NUM', '-'),\n",
       " ('B-EMAIL', '45'),\n",
       " ('B-EMAIL', '##6'),\n",
       " ('I-PHONE_NUM', '-'),\n",
       " ('B-EMAIL', '78'),\n",
       " ('B-EMAIL', '##90'),\n",
       " ('O', '[SEP]')]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "essay = \"\"\"\n",
    "'The consequences of Great Planning', by John Smith\n",
    "\n",
    "The consequences of great planning are many. One of the most important consequences is that it allows us to achieve our goals. When we plan, we are able to think about what we want to achieve and how we are going to achieve it. This helps us to focus on what is important and to avoid wasting time on things that are not important. Planning also helps us to see the big picture and to understand how all the different parts of a project fit together. This can help us to make better decisions and to avoid making mistakes.\n",
    "\n",
    "Another consequence of great planning is that it helps us to be more efficient. When we plan, we can break a big task down into smaller tasks and then work on each task one at a time. This can help us to stay focused and to avoid feeling overwhelmed. Planning can also help us to identify potential problems before they occur and to come up with solutions to those problems. This can help us to save time and to avoid wasting resources.\n",
    "\n",
    "Finally, great planning can help us to be more successful. When we plan, we are able to set clear goals and to create a roadmap for how we are going to achieve those goals. This can help us to stay motivated and to keep moving forward, even when things get tough. Planning can also help us to track our progress and to see how far we have come. This can help us to stay focused and to keep working towards our goals, even when we face obstacles.\n",
    "\n",
    "In conclusion, the consequences of great planning are many. Planning can help us to achieve our goals, to be more efficient, and to be more successful. It can help us to focus on what is important, to avoid wasting time, and to make better decisions. It can help us to stay motivated, to track our progress, and to keep moving forward. In short, great planning is essential for success in any endeavor.\n",
    "\n",
    "email: john.smith@bmail.com\n",
    "phone: 123-456-7890\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "predictions = model(**tokenizer(essay, return_tensors='pt'))\n",
    "predictions = np.argmax(predictions.logits.detach().numpy(), axis=2)\n",
    "\n",
    "bert_tokens = tokenizer.tokenize(essay)\n",
    "bert_tokens = ['[CLS]'] + bert_tokens + ['[SEP]']\n",
    "\n",
    "[(id2label[pred], token) for pred, token in zip(predictions[0], bert_tokens)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2048b70fc4d4074b1551f2400baa060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.010430662892758846,\n",
       " 'eval_precision': 0.10179640718562874,\n",
       " 'eval_recall': 0.22869955156950672,\n",
       " 'eval_f1': 0.1408839779005525,\n",
       " 'eval_accuracy': 0.9983660777738643,\n",
       " 'eval_runtime': 56.1491,\n",
       " 'eval_samples_per_second': 22.832,\n",
       " 'eval_steps_per_second': 2.867}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_data_split[\"train\"],\n",
    "    eval_dataset=encoded_data_split[\"test\"],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 7,\n",
       " 7,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " 14,\n",
       " -100]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_flat['labels'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(encoded_labels['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474 14 0\n",
      "475 14 0\n",
      "476 14 0\n",
      "477 14 7\n",
      "478 14 7\n",
      "['nat', '##hal', '##ie', 'sy', '##lla']\n",
      "['nat', '##hal', '##ie', 'sy', '##lla']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, (l1,l2) in enumerate(zip(data_flat['labels'][0], encoded_labels['labels'][0])):\n",
    "    if l1 != l2:\n",
    "        print(i, l1, l2)\n",
    "\n",
    "print(tokenizer.convert_ids_to_tokens(encoded_labels['input_ids'][0][474:479]))\n",
    "print(tokenizer.convert_ids_to_tokens(data_flat['input_ids'][0][474:479]))\n",
    "\n",
    "data_flat['input_ids'][0] == encoded_labels['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_flat['labels'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
