{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# idea: BERT Tokenizer accepts texts with len> 512, it just gives a warning!!!!!\n",
    "    - use bert tokenizer without truncation\n",
    "    - make chunks\n",
    "    - flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForTokenClassification\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'bert-base-uncased'\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join('..','data', 'raw', 'train.json')\n",
    "\n",
    "data = pd.read_json(data_path)\n",
    "\n",
    "data_dataset = Dataset.from_pandas(data)\n",
    "#data_dataset = data_dataset.remove_columns(['full_text','trailing_whitespace', 'document'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'B-NAME_STUDENT',\n",
       " 1: 'B-EMAIL',\n",
       " 2: 'B-USERNAME',\n",
       " 3: 'B-ID_NUM',\n",
       " 4: 'B-PHONE_NUM',\n",
       " 5: 'B-URL_PERSONAL',\n",
       " 6: 'B-STREET_ADDRESS',\n",
       " 7: 'I-NAME_STUDENT',\n",
       " 8: 'I-EMAIL',\n",
       " 9: 'I-USERNAME',\n",
       " 10: 'I-ID_NUM',\n",
       " 11: 'I-PHONE_NUM',\n",
       " 12: 'I-URL_PERSONAL',\n",
       " 13: 'I-STREET_ADDRESS',\n",
       " 14: 'O',\n",
       " -100: '[PAD]'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS_LIST = ['B-NAME_STUDENT', 'B-EMAIL', 'B-USERNAME', 'B-ID_NUM', 'B-PHONE_NUM', 'B-URL_PERSONAL', 'B-STREET_ADDRESS', 'I-NAME_STUDENT', 'I-EMAIL', 'I-USERNAME', 'I-ID_NUM', 'I-PHONE_NUM','I-URL_PERSONAL','I-STREET_ADDRESS', 'O']\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(LABELS_LIST)}\n",
    "label2id['[PAD]'] = -100\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(example):\n",
    "    \"\"\"\n",
    "    to be used with datasets.map() with batched=False\n",
    "    \n",
    "    Encodes the labels into integers.\n",
    "    \n",
    "    \"\"\"\n",
    "    labels = example['labels']\n",
    "    encoded = [label2id[label] for label in labels]\n",
    "    return {'labels': encoded}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7571d9d8ee44ad08f3d07bf76597feb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_labels_encoded = data_dataset.map(encode_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240d6583bc7248338d39420658743fc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   0,   1,  ..., 998, 999, 999])\n",
      "tensor([  0,   1,   2,  ..., 998, 999, 999])\n",
      "tensor([  0,   0,   1,  ..., 998, 999, 999])\n",
      "tensor([  0,   0,   1,  ..., 998, 999, 999])\n",
      "tensor([  0,   0,   0,  ..., 998, 999, 999])\n",
      "tensor([  0,   0,   1,  ..., 998, 998, 999])\n",
      "tensor([  0,   0,   1,  ..., 805, 806, 806])\n"
     ]
    }
   ],
   "source": [
    "def encode_tokens(examples):\n",
    "    \"\"\"\n",
    "    o be used with datasets.map() with batched=True\n",
    "\n",
    "    Expects a batch of examples to be tokenized and returns the tokenized examples.\n",
    "    \n",
    "    Warning: Doesn't align labels with tokens. If the text is turned into chunks, each chunk will have the labels for the full text in it's row.\n",
    "    \"\"\"\n",
    "    result = tokenizer(examples[\"full_text\"], \n",
    "                       padding=\"max_length\",\n",
    "                       max_length=512, \n",
    "                       truncation=True, \n",
    "                       return_overflowing_tokens=True, \n",
    "                       return_offsets_mapping=True, \n",
    "                       stride=10, \n",
    "                       return_tensors='pt')\n",
    "    \n",
    "    sample_map = result.pop('overflow_to_sample_mapping')\n",
    "    result['overflow_to_sample_mapping'] = sample_map\n",
    "    print(sample_map)\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "tokenized_dataset = data_labels_encoded.map(encode_tokens, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f3b5d12eba40169fd82f11faa9877e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tokenize_and_align(example, sub_word_labeling = False, overlap_size = 0):\n",
    "    \"\"\"\n",
    "    To be used with datasets.map() with batched=False\n",
    "\n",
    "    Takes in \n",
    "        - example : an example from the datasets class\n",
    "        - sub_word_labeling: if True, labels are assigned to subwords of words. If False, subwords (other than the 1st one) are assigned the label -100\n",
    "        - overlap_size: the number of tokens that overlap between two consecutive chunks\n",
    "        \n",
    "    outputs:\n",
    "        - a Dict[]->List with columns:\n",
    "            - of the bert tokenizer output\n",
    "            - encoded labels\n",
    "    \"\"\"\n",
    "\n",
    "    org_labels = example['labels']\n",
    "    tokenized_inputs = tokenizer(example['tokens'], is_split_into_words=True, return_offsets_mapping=True, truncation=True, padding='max_length', max_length=512, return_overflowing_tokens=True, stride=overlap_size, return_tensors='pt')\n",
    "    new_labels = []\n",
    "    # Update the labels with the new tokenization\n",
    "\n",
    "    it = 0\n",
    "\n",
    "    for doc_offset in tokenized_inputs['offset_mapping']:\n",
    "        aligned_labels = []\n",
    "        \n",
    "        if not sub_word_labeling:\n",
    "            arr_offset = np.array(doc_offset)\n",
    "            #making a list of -100 of the same length as the number of BERT tokens in the chunk\n",
    "            arr_aligned_labels = np.ones(len(doc_offset), dtype=int) * (-100)\n",
    "\n",
    "            # calculating the number of labels present in the chunk\n",
    "            nb_labels_in_chunk = len(arr_offset[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)])\n",
    "\n",
    "            # assigning the labels to the corresponding tokens, meaning tokens with offset (0,y)\n",
    "            arr_aligned_labels[(arr_offset[:,0] == 0) & (arr_offset[:,1] != 0)] = org_labels[:nb_labels_in_chunk]\n",
    "\n",
    "            # removing the labels that have been assigned to the chunk from the list of labels\n",
    "            org_labels = org_labels[nb_labels_in_chunk:]\n",
    "\n",
    "            new_labels.append(arr_aligned_labels.tolist())\n",
    "        \n",
    "        \n",
    "        # TODO : subword is slow\n",
    "\n",
    "        # dumb solution with for-loops\n",
    "\n",
    "        if sub_word_labeling:\n",
    "            prev_offset = (1,1)\n",
    "            for x,y in doc_offset:\n",
    "                #if the offset is (0,0), it means it's a special token (CLS, SEP, etc.)\n",
    "                if x == 0 and y == 0:\n",
    "                    aligned_labels.append(-100)\n",
    "                #if the offset is (0,y), it means it's the first subword of a word\n",
    "                elif x == 0 and y !=0:\n",
    "                    aligned_labels.append(org_labels[it])\n",
    "                    it+=1\n",
    "                #if the offset is (x,y) with x!=0, it means it's a subword of a word and the most recent label should be assigned to it\n",
    "                elif x!=0 and prev_offset[1]==x:\n",
    "                    aligned_labels.append(aligned_labels[-1])\n",
    "                prev_offset = (x,y)\n",
    "            new_labels.append(aligned_labels)\n",
    "\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    tokenized_inputs.pop(\"offset_mapping\")\n",
    "    tokenized_inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    return tokenized_inputs\n",
    "    \n",
    "\n",
    "data_chunked_encoded = data_labels_encoded.map(lambda x: tokenize_and_align(x,sub_word_labeling=True), batched=False)\n",
    "#time to tokenize and chunk WITH subword labeling : 1m58s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels aligned with tokens, with subword labeling\n",
      "['thinking', 'for', 'innovation', 'reflex', '##ion', '-', 'av', '##ril', '2021', '-', 'nat', '##hal']\n",
      "[14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"labels aligned with tokens, with subword labeling\")\n",
    "print(tokenizer.convert_ids_to_tokens(data_chunked_encoded['input_ids'][0][0][2:14]))\n",
    "print(data_chunked_encoded['labels'][0][0][2:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d5b7aaf1a142fb9236344d0f8a4294",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_chunked_encoded_sub_labeling_false = data_labels_encoded.map(lambda x: tokenize_and_align(x,sub_word_labeling=False), batched=False)\n",
    "#time to tokenize and chunk WITHOUT subword labeling : 46s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels aligned with tokens, without subword labeling\n",
      "['thinking', 'for', 'innovation', 'reflex', '##ion', '-', 'av', '##ril', '2021', '-', 'nat', '##hal']\n",
      "[14, 14, 14, 14, -100, 14, 14, -100, 14, 14, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "print(\"labels aligned with tokens, without subword labeling\")\n",
    "print(tokenizer.convert_ids_to_tokens(data_chunked_encoded_sub_labeling_false['input_ids'][0][0][2:14]))\n",
    "print(data_chunked_encoded_sub_labeling_false['labels'][0][0][2:14])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "#flatten the dataset\n",
    "data_flat = {}\n",
    "\n",
    "keys = ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
    "\n",
    "for key in keys:\n",
    "    data_flat[key] = reduce(lambda x,y: x+y, data_chunked_encoded[key])\n",
    "\n",
    "data_flat = Dataset.from_dict(data_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_flat['labels'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_flat['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_preserve_labels(examples):\n",
    "    labels = []\n",
    "    tokenized_sentence = []\n",
    "    for word, label in zip(examples['tokens'], examples['labels']):\n",
    "        #tokenizes the word using BERT's subword tokenizer\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "        #adds the same label to all the subwords of the word\n",
    "        labels.extend([label] * n_subwords)\n",
    "    examples['tokens'] = tokenized_sentence\n",
    "    examples['labels'] = labels\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9251aa5055d4909bfed2c7a68cf7c46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_data = data_dataset.map(tokenize_and_preserve_labels,batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk(examples, block_size=510, sliding_window=False):\n",
    "    tokenized_sentences = []\n",
    "    labels = []\n",
    "    for i in range(0, len(examples['tokens']), block_size):\n",
    "        chunk_token = examples['tokens'][i:i+block_size]\n",
    "        chunk_label = examples['labels'][i:i+block_size]\n",
    "        if len(chunk_token) < block_size:\n",
    "            chunk_token += ['[PAD]'] * (block_size - len(chunk_token))\n",
    "            chunk_label += ['[PAD]'] * (block_size - len(chunk_label))\n",
    "        tokenized_sentences.append(chunk_token)\n",
    "        labels.append(chunk_label)\n",
    "    return {'tokens': tokenized_sentences, 'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d30d53b2d12487fb6cd6435413a6363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6807 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chunked_data = tokenized_data.map(chunk, batched=False)\n",
    "chunked_data = chunked_data.remove_columns(['document', 'full_text','trailing_whitespace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "flat_chunks_tokens = list(reduce(lambda x, y: x + y, chunked_data['tokens'], []))\n",
    "flat_chunks_labels = list(reduce(lambda x, y: x + y, chunked_data['labels'], []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_flattened_data = Dataset.from_dict({'tokens': flat_chunks_tokens, 'labels': flat_chunks_labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the '[PAD]' label to be encoded as -100 so that it's ignored by the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'B-NAME_STUDENT': 0,\n",
       "  'B-EMAIL': 1,\n",
       "  'B-USERNAME': 2,\n",
       "  'B-ID_NUM': 3,\n",
       "  'B-PHONE_NUM': 4,\n",
       "  'B-URL_PERSONAL': 5,\n",
       "  'B-STREET_ADDRESS': 6,\n",
       "  'I-NAME_STUDENT': 7,\n",
       "  'I-EMAIL': 8,\n",
       "  'I-USERNAME': 9,\n",
       "  'I-ID_NUM': 10,\n",
       "  'I-PHONE_NUM': 11,\n",
       "  'I-URL_PERSONAL': 12,\n",
       "  'I-STREET_ADDRESS': 13,\n",
       "  'O': 14,\n",
       "  '[PAD]': -100},\n",
       " {0: 'B-NAME_STUDENT',\n",
       "  1: 'B-EMAIL',\n",
       "  2: 'B-USERNAME',\n",
       "  3: 'B-ID_NUM',\n",
       "  4: 'B-PHONE_NUM',\n",
       "  5: 'B-URL_PERSONAL',\n",
       "  6: 'B-STREET_ADDRESS',\n",
       "  7: 'I-NAME_STUDENT',\n",
       "  8: 'I-EMAIL',\n",
       "  9: 'I-USERNAME',\n",
       "  10: 'I-ID_NUM',\n",
       "  11: 'I-PHONE_NUM',\n",
       "  12: 'I-URL_PERSONAL',\n",
       "  13: 'I-STREET_ADDRESS',\n",
       "  14: 'O',\n",
       "  -100: '[PAD]'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'labels'],\n",
       "    num_rows: 12812\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_flattened_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(map(lambda x: ' '.join(x), chunked_flattened_data['tokens'][:3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = list(map(lambda x: x.replace(' ##', ''), x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"design thinking for innovation reflexion - avril 2021 - nathalie sylla challenge & selection the tool i use to help all stakeholders finding their way through the complexity of a project is the mind map . what exactly is a mind map ? according to the definition of buzan t . and buzan b . ( 1999 , dessine - moi l ' intelligence . paris : les editions d ' organisation . ) , the mind map ( or heuristic diagram ) is a graphic representation technique that follows the natural functioning of the mind and allows the brain ' s potential to be released . cf annex1 this tool has many advantages : • it is accessible to all and does not require significant material investment and can be done quickly • it is scalable • it allows categorization and linking of information • it can be applied to any type of situation : notetaking , problem solving , analysis , creation of new ideas • it is suitable for all people and is easy to learn • it is fun and encourages exchanges • it makes visible the dimension of projects , opportunities , interconnections • it synthesizes • it makes the project understandable • it allows you to explore ideas the creation of a mind map starts with an idea / problem located at its center . this starting point generates ideas / work areas , incremented around this center in a radial structure , which in turn is completed with as many branches as new ideas . this tool enables creativity and logic to be mobilized , it is a map of the thoughts . creativity is enhanced because participants feel comfortable with the method . application & insight i start the process of the mind map creation with the stakeholders standing around a large board ( white or paper board ) . in the center of the board , i write and highlight the topic to design . through a series of questions , i guide the stakeholders in modelling the mind map . i adapt the series of questions according to the topic to be addressed . in the type of questions , we can use : who , what , when , where , why , how , how much . the use of the “ why ” is very interesting to understand the origin . by this way , the interviewed person frees itself from paradigms and thus dares to propose new ideas / ways of functioning . i plan two hours for a workshop . design thinking for innovation reflexion - avril 2021 - nathalie sylla after modelling the mind map on paper , i propose to the participants a digital visualization of their work with the addition of color codes , images and interconne\",\n",
       " '##ctions . this second workshop also lasts two hours and allows the mind map to evolve . once familiarized with it , the stakeholders discover the power of the tool . then , the second workshop brings out even more ideas and constructive exchanges between the stakeholders . around this new mind map , they have learned to work together and want to make visible the untold ideas . i now present all the projects i manage in this type of format in order to ease rapid understanding for decision - makers . these presentations are the core of my business models . the decision - makers are thus able to identify the opportunities of the projects and can take quick decisions to validate them . they find answers to their questions thank to a schematic representation . approach what i find amazing with the facilitation of this type of workshop is the participants commitment for the project . this tool helps to give meaning . the participants appropriate the story and want to keep writing it . then , they easily become actors or sponsors of the project . a trust relationship is built , thus facilitating the implementation of related actions . design thinking for innovation reflexion - avril 2021 - nathalie sylla annex 1 : mind map shared facilities project [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " 'diego estrada design thinking assignment visualization tool challenge & selection the elderly were having a hard time adapting to the changes we brought in our bank . as a result of a poorly implemented linear solution , a more customer centric approach was needed . after learning about design thinking in this course , we decided to apply it to solve this problem . the visualization tool allowed the team to create a dynamic presentation using diagrams , figures and drawings on the go that really resonated among the stakeholders . previous to this change , none of our solutions seemed to be adequate for them , but the new implementation created a different type of connection with them that helped them understand the problem in the way the team and i did . application the process starts in the prep time . the team uses a series of tools and software to develop a presentation using the surveys gathered during research and the solutions we created during the process . the use of graphs to quickly show statistics in a fully visual way , rather than verbally was a game changer . after having a presentation prepared , the team hands an activity to the stakeholders , where the solutions discussed previously appear . nonetheless , the solutions need more work to them . after this . the stakeholders are asked to help complete the solutions while the team and i create diagrams on a blackboard to represent how their suggestions would impact on this specific problem . the use of a group activity strengthens the bond between the company and their investors . it makes them feel like they take part and help solve the problems as well as show how customer centric the solutions are . every complaint and suggestion from customers are read and evaluated using the graph shown in the course ( involving : can we do it ? can we afford it ? … ) . the finalization of this activity leaves the team and the stakeholders on the same page . it allows them to completely understand and feel part of the solution and also gives them the chance to ask better questions , which eases the work of the team . insight & approach the use of this method created a new workflow in the design team . it increased the productivity and the success rate as well as the customer / stakeholders satisfaction . the use of the visualization tool created an engaged group of people who work together to diego estrada find a solution based on their customer satisfaction . this solution is later revised and tweaked with the help of the stakeholders who are deeply involved in the process . presentations , graphics , and activities have added a huge increase in satisfaction . as a company we also learnt that engaging different areas can be difficult because of the varying levels']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] design thinking for innovation reflexion - avril 2021 - nathalie sylla challenge & selection the tool i use to help all stakeholders finding their way through the complexity of a project is the mind map. what exactly is a mind map? according to the definition of buzan t. and buzan b. ( 1999, dessine - moi l'intelligence. paris : les editions d'organisation. ), the mind map ( or heuristic diagram ) is a graphic representation technique that follows the natural functioning of the mind and allows the brain's potential to be released. cf annex1 this tool has many advantages : • it is accessible to all and does not require significant material investment and can be done quickly • it is scalable • it allows categorization and linking of information • it can be applied to any type of situation : notetaking, problem solving, analysis, creation of new ideas • it is suitable for all people and is easy to learn • it is fun and encourages exchanges • it makes visible the dimension of projects, opportunities, interconnections • it synthesizes • it makes the project understandable • it allows you to explore ideas the creation of a mind map starts with an idea / problem located at its center. this starting point generates ideas / work areas, incremented around this center in a radial structure, which in turn is completed with as many branches as new ideas. this tool enables creativity and logic to be mobilized, it is a map of the thoughts. creativity is enhanced because participants feel comfortable with the method. application & insight i start the process of the mind map creation with the stakeholders standing around a large board ( white or paper board ). in the center of the board, i write and highlight the topic to design. through a series of questions, i guide the stakeholders in modelling the mind map. i adapt the series of questions according to the topic to be addressed. in the type of questions, we can use : who, what, when, where, why, how, how much. the use of the “ why ” is very interesting to understand the origin. by this way, the interviewed person frees itself from paradigms and thus dares to propose new ideas / ways of functioning. i plan two hours for a workshop. design thinking for innovation reflexion - avril 2021 - nathalie sylla after modelling the mind map on paper, i propose to the participants a digital visualization of their work with the addition of color codes, images and interconne [SEP]\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer(x2[0])['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tokens(example):\n",
    "    detokenized = list(map(lambda x: ' '.join(x), example['tokens']))\n",
    "    detokenized = list(map(lambda x: x.replace(' ##', ''), detokenized))\n",
    "    encoded = tokenizer(detokenized, truncation=True, is_split_into_words = False, return_tensors='pt')\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0469a57fac694ab1aea1852b31bd69f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_data = chunked_flattened_data.map(encode_tokens, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e78d2cafef74b2e85851f4f1b3b2881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "encoded_data = encoded_data.map(encode_labels, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 12812\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['-', 'nat', '##hal', '##ie', 'sy', '##lla', 'challenge', '&', 'selection'],\n",
       " [14, 0, 0, 0, 7, 7, 14, 14, 14],\n",
       " [1011, 14085, 8865, 2666, 25353, 4571, 4119, 1004, 4989])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data['tokens'][0][10:19], encoded_data['labels'][0][11:20], encoded_data['input_ids'][0][11:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data_split = encoded_data.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- nathalie sylla challenge & selection'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_data['input_ids'][0][11:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    \n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [LABELS_LIST[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [LABELS_LIST[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME, num_labels=len(id2label), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b6104de3514d16999609d5029dbee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4326 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0271, 'learning_rate': 4.4220989366620436e-05, 'epoch': 0.35}\n",
      "{'loss': 0.0091, 'learning_rate': 3.844197873324087e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5336a455ce3941c98b355ce7bfb88ce7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.002221583155915141, 'eval_precision': 0.7661691542288557, 'eval_recall': 0.6984126984126984, 'eval_f1': 0.7307236061684459, 'eval_accuracy': 0.9994324666757416, 'eval_runtime': 52.3205, 'eval_samples_per_second': 24.503, 'eval_steps_per_second': 3.077, 'epoch': 1.0}\n",
      "{'loss': 0.0034, 'learning_rate': 3.266296809986131e-05, 'epoch': 1.04}\n",
      "{'loss': 0.0023, 'learning_rate': 2.688395746648174e-05, 'epoch': 1.39}\n",
      "{'loss': 0.0021, 'learning_rate': 2.1104946833102173e-05, 'epoch': 1.73}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62976cf570ca467db402d0e4e4a2bdf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0013160010566934943, 'eval_precision': 0.8074245939675174, 'eval_recall': 0.7891156462585034, 'eval_f1': 0.7981651376146789, 'eval_accuracy': 0.9995937215317117, 'eval_runtime': 52.421, 'eval_samples_per_second': 24.456, 'eval_steps_per_second': 3.071, 'epoch': 2.0}\n",
      "{'loss': 0.0013, 'learning_rate': 1.5325936199722607e-05, 'epoch': 2.08}\n",
      "{'loss': 0.0005, 'learning_rate': 9.546925566343042e-06, 'epoch': 2.43}\n",
      "{'loss': 0.0007, 'learning_rate': 3.7679149329634766e-06, 'epoch': 2.77}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abe89fd6abb494eb38f4c912509d99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/projet-en-tal/lib/python3.11/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0011914735659956932, 'eval_precision': 0.8928571428571429, 'eval_recall': 0.7369614512471655, 'eval_f1': 0.8074534161490683, 'eval_accuracy': 0.9996481712233379, 'eval_runtime': 52.1851, 'eval_samples_per_second': 24.566, 'eval_steps_per_second': 3.085, 'epoch': 3.0}\n",
      "{'train_runtime': 11353.2091, 'train_samples_per_second': 3.047, 'train_steps_per_second': 0.381, 'train_loss': 0.0054270451405342675, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4326, training_loss=0.0054270451405342675, metrics={'train_runtime': 11353.2091, 'train_samples_per_second': 3.047, 'train_steps_per_second': 0.381, 'train_loss': 0.0054270451405342675, 'epoch': 3.0})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"model_baseline_v0.1\",\n",
    "#     learning_rate=2e-3,\n",
    "#     per_device_train_batch_size=20,\n",
    "#     per_device_eval_batch_size=20,\n",
    "#     num_train_epochs=2,\n",
    "#     weight_decay=0.01,\n",
    "#     evaluation_strategy=\"epoch\",\n",
    "#     save_strategy=\"epoch\",\n",
    "#     load_best_model_at_end=True,\n",
    "# )\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"model/test_trainer\", evaluation_strategy=\"epoch\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_data_split[\"train\"],\n",
    "    eval_dataset=encoded_data_split[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    #data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('model/model_baseline_v0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen = 'my name is John Smith and my email is john.smith@gmail.com. I live in 1234 Elm Street. My phone number is 123-456-7890. My username is johnsmith123. My ID number is 123456789. My personal website is www.johnsmith.com.'\n",
    "\n",
    "model = model.to('cpu')\n",
    "\n",
    "predictions = model(**tokenizer(sen, return_tensors='pt'))\n",
    "predictions = np.argmax(predictions.logits.detach().numpy(), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokens = tokenizer.tokenize(sen)\n",
    "bert_tokens = ['[CLS]'] + bert_tokens + ['[SEP]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', '[CLS]'),\n",
       " ('O', 'my'),\n",
       " ('O', 'name'),\n",
       " ('O', 'is'),\n",
       " ('B-NAME_STUDENT', 'john'),\n",
       " ('I-NAME_STUDENT', 'smith'),\n",
       " ('O', 'and'),\n",
       " ('O', 'my'),\n",
       " ('O', 'email'),\n",
       " ('O', 'is'),\n",
       " ('B-NAME_STUDENT', 'john'),\n",
       " ('B-EMAIL', '.'),\n",
       " ('B-EMAIL', 'smith'),\n",
       " ('B-EMAIL', '@'),\n",
       " ('B-EMAIL', 'gma'),\n",
       " ('B-EMAIL', '##il'),\n",
       " ('B-EMAIL', '.'),\n",
       " ('B-EMAIL', 'com'),\n",
       " ('O', '.'),\n",
       " ('O', 'i'),\n",
       " ('O', 'live'),\n",
       " ('O', 'in'),\n",
       " ('O', '123'),\n",
       " ('O', '##4'),\n",
       " ('O', 'elm'),\n",
       " ('O', 'street'),\n",
       " ('O', '.'),\n",
       " ('O', 'my'),\n",
       " ('O', 'phone'),\n",
       " ('O', 'number'),\n",
       " ('O', 'is'),\n",
       " ('B-ID_NUM', '123'),\n",
       " ('I-PHONE_NUM', '-'),\n",
       " ('B-ID_NUM', '45'),\n",
       " ('B-ID_NUM', '##6'),\n",
       " ('I-PHONE_NUM', '-'),\n",
       " ('B-ID_NUM', '78'),\n",
       " ('B-ID_NUM', '##90'),\n",
       " ('O', '.'),\n",
       " ('O', 'my'),\n",
       " ('O', 'user'),\n",
       " ('O', '##name'),\n",
       " ('O', 'is'),\n",
       " ('O', 'johns'),\n",
       " ('B-EMAIL', '##mith'),\n",
       " ('B-ID_NUM', '##12'),\n",
       " ('B-ID_NUM', '##3'),\n",
       " ('O', '.'),\n",
       " ('O', 'my'),\n",
       " ('O', 'id'),\n",
       " ('O', 'number'),\n",
       " ('O', 'is'),\n",
       " ('B-ID_NUM', '123'),\n",
       " ('B-ID_NUM', '##45'),\n",
       " ('B-ID_NUM', '##6'),\n",
       " ('B-ID_NUM', '##7'),\n",
       " ('B-ID_NUM', '##8'),\n",
       " ('B-ID_NUM', '##9'),\n",
       " ('O', '.'),\n",
       " ('O', 'my'),\n",
       " ('O', 'personal'),\n",
       " ('O', 'website'),\n",
       " ('O', 'is'),\n",
       " ('B-URL_PERSONAL', 'www'),\n",
       " ('B-URL_PERSONAL', '.'),\n",
       " ('B-URL_PERSONAL', 'johns'),\n",
       " ('B-EMAIL', '##mith'),\n",
       " ('B-EMAIL', '.'),\n",
       " ('B-EMAIL', 'com'),\n",
       " ('O', '.'),\n",
       " ('O', '[SEP]')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(id2label[pred], token) for pred, token in zip(predictions[0], bert_tokens)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
